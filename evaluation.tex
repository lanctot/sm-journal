%!TEX root = sm-journal.tex

We now turn to thorough experimental evaluation of the described algorithms.
We analyze both offline and online case on a collection of games inspired by games used for experimental evaluation in previous works, and randomly generated games.
After describing the rules of the games, we present the results for the offline strategy computation case and we follow with the online game playing.

\subsection{Experimental Settings}

%TODO: Remove 5s results, add paragraph + graph summary

We start with an experimental evaluation of a well-known example of Biased Rock, Paper, Scissors~\cite{Shafiei09} that often serves
as an example that MCTS with UCT selection function does not converge to a Nash equilibrium.
%\bbosansky{In the description of UCT we need to suggest that there are two things how to do it (deterministically, randomly) -- in the experiments only refer to this.} done
We reproduce this experiment and show the differences in performance of the sampling algorithms -- primarily the impact of randomization in UCT.
Then, we compare the offline performance of the algorithms on other domains.
For each domain, we first analyze the exact algorithms and measure the computation time taken to solve a particular instance of the game.
We compute mean out of at least $30$ runs for each game settings.
Afterwards, we analyze the convergence of the approximative algorithms.
At the specified time step the algorithm produces strategies $(\sigma_1,\sigma_2)$, and using best responses we compute
$\textrm{error}(\sigma_1,\sigma_2) = \max_{\sigma_1' \in \Sigma_1} \bE_{z \sim (\sigma_1',\sigma_2)}[u_1(z)]
                                   + \max_{\sigma_2' \in \Sigma_2} \bE_{z \sim (\sigma_1,\sigma_2')}[u_2(z)]$,
which is equal to $0$ at a Nash equilibrium.
In each offline convergence setting, the reported values are means out of at least $20$ runs of each sampling algorithm on a single instance of the game.
We compared at least $3$ different settings for the exploration parameter and present the result only for the best exploration parameter.
For OOS, Exp3, and RM the best values for the parameters were almost always $0.6$, $0.1$, and $0.1$.
The only exception was Goofspiel with chance nodes, where both Exp3 and RM converge more quickly with parameter set to $0.3$.
For UCT, however, the performance was quite different and we give the values for UCT for each domain.
% mlanctot: please check the definition of error
%\vlisy{is the 20 runs enough?}\bbosansky{this is the lowest number, some games (e.g., Goofspiel) had more samples (40)}

Finally, we turn to the comparison of the algorithms in the online setting and we present head-to-head tournament on each domain.
We used larger instances of games and calculated win-rate for each algorithm.
We use two different restrictions to computation time per move: 1 second and 5 seconds.
% mlanctot: we do not have to justify this choice
%The first one is more common in previous works focused on online comparison of the algorithms~\cite{XXX}\bbosansky{Reference missing -- ideally several works with different authors that use this settings.}, second one allows us to see the difference if the algorithms have more computation time.
As indicated before, the algorithms based on the backward induction need to use domain-specific evaluation function in the online settings.
This may give these algorithms an advantage if the evaluation function is very precise.
Therefore, for selected domains we also run sampling-based algorithms with an evaluation function to compare the algorithms in a more fair settings.
Reported results are mean of at least $1000$ matches for each pair of algorithms.
%Where necessary, we run additional experiments to obtain statistically significant results.

Each of the described algorithms was implemented in a generic framework for modeling and solving extensive-form games\footnote{Source code is available at \texttt{http://agents.felk.cvut.cz/topics/Computational\_\newline game\_theory}. We use IBM CPLEX 12.5 to solve the linear programs.}.
We are interested in the performance of the algorithms and their ability to find or approximate the optimal behavior.
Therefore, with the exception of the evaluation function used in selected online experiments, no algorithm is using any domain-specific knowledge.


\subsection{Domains}\label{sec:eval:domains}

In this subsection, we describe the six domains used in our experiments.
The games in our collection differ in characteristics, such as the number of available actions for each player (i.e., the branching factor), maximal depth, and number of possible utility values.
Moreover, the games also differ in the \emph{randomization factor} -- i.e., how often it is necessary to use mixed strategies and whether this randomization occurs at the beginning of the game, near the end of the game, or it is spread throughout the whole course of the game.

For each domain we also describe the evaluation function used in the online experiments.
Note that we are not seeking the best-performing algorithm for a particular game; hence, we have not aimed for the most accurate evaluation functions for each game.
We intentionally use evaluation functions of different quality that allow us to compare the differences between the algorithms from this perspective as well.

\paragraph{\textbf{Biased Rock, Paper, Scissors}}

BRPS is a payoff-skewed version of the one-shot game Rock, Paper, Scissors shown in
Figure~\ref{fig:brps}. This game was introduced in \cite{Shafiei09}, and it was shown that the visit count
distribution of UCT does converge to a fixed balanced situation, but not one that
corresponds to the optimal mixed strategy of $(\frac{1}{16},\frac{10}{16},\frac{5}{16})$.
% mlanctot: should be done for the whole section
%\vlisy{unify the use of UCT/DUCT}

%\item[Goofspiel] is a card game where each player gets 13 cards marked 1-13, and there is a face down
%point-card stack (also 1-13). Every turn, the {\it upcard} (top card of the point-card stack is turned face up,
%Each player chooses a {\it bid} card from their hand simultaneously.
%The player with the higher bid takes the upcard. The bid cards are then discarded and a new round starts.
%At the end of 13 rounds, the player with the highest number of points wins, a tie ends in a draw.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{c|c|c|c|}
 \multicolumn{1}{c}{~} & \multicolumn{1}{c}{r}  &  \multicolumn{1}{c}{p} &  \multicolumn{1}{c}{s}\\\cline{2-4}
R &  0  & -25& 50\\\cline{2-4}
P &  25 &  0 & -5\\\cline{2-4}
S & -50 &  5 &  0\\\cline{2-4}
\end{tabular}
%\includegraphics[scale=1.0]{figures/biased-rps}
\end{center}
\caption{Biased Rock, Paper, Scissors matrix game from~\cite{Shafiei09}. \label{fig:brps}}
\end{figure}

\paragraph{\textbf{Goofspiel}}
Goofspiel is a card game that appears in many works dedicated to simultaneous move games (e.g., \cite{Ross71Goofspiel,Rhoads12Computer,Saffidine12SMAB,Lanctot13Goofspiel}).
There are $3$ identical decks of cards with values $\{0,\dots, (d-1)\}$ (one for nature and one for each player), where $d$ is a parameter of the game (classical Goofspiel is played with $13$ cards).
The game is played in rounds: at the beginning of each round, nature reveals one card from its deck and both players bid for the card by simultaneously selecting (and removing) a card from their hands.
A player that selects a higher card wins the round and receives a number of points equal to the value of the nature's card.
In case both players select the card with the same value, the nature's card is discarded.
When there are no more cards to be played, the winner of the game is chosen based on the sum of card values he received during the whole game.

There are two parameters of the game that can be altered to create four different variants of Goofspiel.
The first parameter relates to nature.
We can use an assumption made in the previous work that used Goofspiel as a benchmark for evaluation of the exact offline algorithms \cite{Saffidine12SMAB}, where the order of the cards of nature is randomly chosen at the beginning of the game and it is known to both players.
We refer to this setting as with the fixed ordering of cards of nature.
Alternatively, we can treat nature in a standard way, where the players do not know the exact future sequence of the cards of nature.
The games are fairly similar in terms of performance of the algorithms, however, the second variant induces considerably larger game tree.
The second parameter relates to the utility functions.
\reviewchange{Either we treat the game as a win-tie-lose game (i.e., the players receive utility from set $\lbrace -1, 0, 1 \rbrace$), or the utility values for the players are equal to the points they gain during the game.}

Goofspiel forms game trees with interesting properties.
First unique feature is that the number of actions for each player is uniformly decreasing by $1$ with the depth.
Secondly, algorithms must randomize in NE strategies, and this randomization is present throughout the whole course of the game.
As an example, the following table depicts the number of states with pure strategies and mixed strategies for each depth in a subgame-perfect NE calculated by backward induction for Goofspiel with $5$ cards and fixed sequence of cards of nature:

\vspace{0.1cm}

\begin{center}
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline Depth & 0 & 1 & 2 & 3 & 4 \\
\hline Pure  & 0 & 17 & 334 & 3354 & 14400 \\
\hline Mixed & 1 &  8 &  66 &  246 & 0 \\
\hline
\end{tabular}
\end{center}

\vspace{0.1cm}

We can see that the relative number of states with mixed strategies slowly decreases, however, players need to mix throughout the whole game.
In the last round, each player has only a single card; hence, there cannot be any mixed strategy.

%The evaluation function used in Goofspiel takes into consideration two components: (1) the difference in the current score for both players weighted by the current round in the game, and (2) the remaining cards in the deck weighted by a chance of winning these cards depending on the remaining cards on hand for each player. Formally, let $p_i$, $r$, $c_i$ be player $i$'s score (points), the current round, and the sum of values of player $i$'s remaining cards, respectively:

Our hand-tuned evaluation function used in Goofspiel takes into consideration the remaining cards in the deck weighted by a chance of winning these cards depending on the remaining cards on hand for each player.
Moreover, if the position is clearly winning for one of the players (there is not enough cards to change the current score), the evaluation function is set to maximal (or minimal) value.
For the win-tie-loose case, the formal definition follows ($c_i$ are the sum of values the remaining cards of player $i$):

\[
eval(s) = \left\{ \begin{array}{ll}
  u_1(s) & \mbox{if $c_1 + c_2 = 0$ };\\
  \tanh\left(\frac{c_1 - c_2}{c_1 + c_2}\cdot\frac{c_c}{0.5 \cdot d(d+1)}\right) & \mbox{otherwise.} \end{array} \right.
\]


%eval(s) = \tanh\left(\frac{\textrm{score}_1 - \textrm{score}_2}{\textrm{score}_1 + \textrm{score}_2}\cdot\frac{\textrm{round}}{d} + \frac{\textrm{remCards}_1 - \textrm{remCards}_2}{\textrm{remCards}_1 + \textrm{remCards}_2}\cdot\frac{\textrm{remCards}_c}{0.5\cdot d(d+1)}\right)
%\[ |x| = \left\{ \begin{array}{ll}
%         x & \mbox{if $x \geq 0$};\\
%        -x & \mbox{if $x < 0$}.\end{array} \right. \]
%\[
%eval(s) = \left\{ \begin{array}{ll}
%  0      & \mbox{if $p_1 + p_2 = 0$ (start)};\\
%  u_1(s) & \mbox{if $c_1 + c_2 = 0$ (end)};\\
%  \tanh\left(\frac{p_1 - p_2}{p_1 + p_2}\cdot\frac{r}{d} + \frac{c_1 - c_2}{c_1 + c_2}\cdot\frac{c_c}{0.5 \cdot d(d+1)}\right) & \mbox{otherwise.} \end{array} \right.
%\]
%$eval(s)$ is equal to $0$ in the beginning of the game (i.e., when $p_1 + p_2$ is $0$) and to the utility value of the game at the end (i.e., when $c_1 + c_2$ is $0$).
For the win-tie-loose case we use $\tanh$ to scale the evaluation function into the interval $[-1,1]$; this function is omitted in the exact point case.
%Moreover, if the position is clearly winning for one of the players (there is not enough cards to change the current score), the evaluation function is set to $1$, or $-1$.
%\item[Oshi-Zumo]$(N,K,M)$ is a wrestling simulation game played on a discrete single-dimensional grid with
%$2K+1$ positions, where each player starts with $N$ coins~\cite{buro2003}. A wrestler token begins in the middle
%position. Every turn,
%each player bids $b \ge M$ coins. The coins bid are then discarded and the player bidding the most coins pushes the
%wrestler one position closer to the goal for that player.

\paragraph{\textbf{Oshi-Zumo}}

Oshi-Zumo is a board game analyzed from the perspective of computational game theory in \cite{buro2003}.
There are two players in the game, both starting with $N$ coins, and there is a playing board represented as a one-dimensional playing field with $2K+1$ locations (indexed $0, \ldots, 2K$).
At the beginning, there is a stone (or a wrestler) located in the center of the playing field (i.e., at position $K$).
During each move, both players simultaneously place their bid from the amount of coins they have (but at least $M$ if they still have some coins).
Afterwards, the bids are revealed, both bids are subtracted from the number of coins of the players, and the highest bidder can push the wrestler one location towards the opponent's side.
If the bids are the same, the wrestler does not move.
The game proceeds until the money runs out for both players, or the wrestler is pushed out of the field.
The winner is determined based on the position of the wrestler -- the player in whose half the wrestler is located loses the game.
If the final position of the wrestler is the center, the game is a draw.
\reviewchange{Again, we have examined two different settings of the utility values: they are either restricted to win-tie-loose values $\lbrace -1, 0, 1 \rbrace$, or they correspond to the relative position of the wrestler $\lbrace \textrm{wrestler} - K, K - \textrm{wrestler} \rbrace$.
In the experiments we varied the number of coins and parameter~$K$.}

Many instances of the Oshi-Zumo game have pure Nash equilibrium.
With the increasing number of the coins the game necessarily need to use mixed strategies, however, mixing is typically required only at the beginning of the game.
As an example, the following table depicts the number of states with pure strategies and mixed strategies in a subgame-perfect NE calculated by backward induction for Oshi-Zumo with $N=10$ coins, $K=3$, and minimal bid $M=1$. The results show that there are very few states where mixed strategies are required, and they are present only at the beginning of the game tree. Also note, that contrary to Goofspiel, not all branches have the same length.

\vspace{0.1cm}

\begin{center}
\small
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline Depth & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
\hline Pure  & 1 & 98 & 2012 & 14767 & 48538 & 79926 & 69938 & 33538 & 8351 & 861\\
\hline Mixed & 0 &  1 &  4 &  17 & 8 & 0 & 0 & 0 & 0 & 0 \\
\hline
\end{tabular}
\end{center}

\vspace{0.1cm}

The evaluation function used in Oshi-Zumo takes into consideration two components: (1) the current position of the wrestler and, (2) the remaining coins for each player. Formally:
$$
eval(s) = \tanh\left(\frac{b}{2}+\frac{1}{3}\left(\frac{\textrm{coins}_1 - \textrm{coins}_2}{M} + \textrm{wrestler} - K\right)\right)
$$
where $b$ equals to 1 if $\textrm{coins}_1 \geq \textrm{coins}_2$ and $wrestler \geq K$, and at least one of the inequalities is strict;
$b$ equals to $-1$, if $\textrm{coins}_1 \leq \textrm{coins}_2$ and $wrestler \leq K$, and at least one of the inequalities is strict.
Again, we use $\tanh$ to scale the evaluation function into the interval $[-1,1]$.

\paragraph{\textbf{Pursuit Evasion Games}}

Another important class of games is pursuit-evasion games (for example, see~\cite{nguyen2013monte}).
There is a single evader and a pursuer that controls 2 pursuing units on a four-connected grid in our pursuit-evasion game.
Since all units move simultaneously, the game has larger branching factor than Goofspiel (up to $16$ actions for the pursuer).
The evader wins, if she successfully avoids the units of the pursuer for the whole game; pursuer wins, if her units successfully capture the evader. The evader is captured if either her position is the same as the position of a pursuing unit, or the evader used the same edge as a pursuing unit (in the opposite direction).
The game is win-loss and the players receive utility from set $\lbrace -1, 1 \rbrace$.
We use $3$ different square grid-graphs (with the size of a side 4, 5, and 10 nodes) for the experiments without any obstacles or holes.
In the experiments we varied the length of the game $d$ and we altered the starting positions of the players (the distance between the pursuers and the evader was always at most $\left\lfloor\frac{2}{3} d\right\rfloor$ moves, in order to provide a possibility for the pursuers to capture the evader).

Similarly to Oshi-Zumo, many instances of pursuit-evasion games have a pure Nash equilibrium.
However, the mixing can be required towards the actual end of the game in order to capture the evader.
Therefore, depending on the length of the game and the distance between the units, there might be many states that do not require mixed strategies (units of the pursuers are simply going towards the evader).
Once the units are close to each other, the game may require mixed strategies for final coordination.
This can be seen on our small example on a graph $4\times4$ nodes and depth $5$:

\vspace{0.1cm}

\begin{center}
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline Depth & 0 & 1 & 2 & 3 & 4 \\
\hline Pure  & 1 & 12 & 261 & 7656 & 241986 \\
\hline Mixed & 0 & 0 & 63 & 1008 & 6726 \\
\hline
\end{tabular}
\end{center}

\vspace{0.1cm}

The evaluation function used in pursuit-evasion games takes into consideration the distance between the units of the pursuer and evader (denoted $\textrm{distance}_j$ for the distance in moves of the game between the $j^{th}$ unit of the pursuer and the evader). Formally:
$$
eval(s) = \frac{\min(\textrm{distance}_1,\textrm{distance}_2) + 0.01\cdot\max(\textrm{distance}_1,\textrm{distance}_2)}{1.01 \cdot (w+l)}
$$
where $w$ and $l$ are dimensions of the grid graph.

\paragraph{\textbf{Random/Synthetic Games}}
We also use randomly generated games to be able to experiment with additional parameters of the game, mainly larger utility values and their correlation.
In randomly generated games, we fixed the number of actions that players can play in each stage to $4$ and $5$ (the results were similar for different branching factors) and we varied the depth of the game tree.
We use $2$ different methods for randomly assigning the utility values to the terminal states of the game:
(1) the utility values are uniformly selected from the interval $\left[0,1\right]$;
(2) we randomly assign either $-1$, $0$, or $+1$ value to each joint action (pair of actions) and the utility value in a leaf is a sum of all values on edges on the path from the root of the game tree to the leaf.
The first method produces extremely difficult games for pruning using either alpha-beta search, or double-oracle methods, since there is no correlation between actions and utility values in sibling leaves.
The latter method is based on random \emph{T-games} \cite{smith1995}, that create more realistic games using the intuition of good and bad moves.

Randomly generated games represent games that require mixed strategies in most of the states.
This holds even for games of the second type with correlated utility values in the leaves.
The following table shows the number of states depending on the depth for randomly generated game of depth $5$ with $4$ actions available to both players in each state:

\vspace{0.1cm}

\begin{center}
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline Depth & 0 & 1 & 2 & 3 & 4 \\
\hline Pure  & 0 & 2 & 29 & 665 & 20093 \\
\hline Mixed & 1 & 14 & 227 & 3431 & 45443 \\
\hline
\end{tabular}
\end{center}

\vspace{0.1cm}

Only the second type of randomly generated games is used in the online setting.
The evaluation function used in this case is calculated similarly as the utility value and it is equal to the sum of values on the edges from the root to the current node.

\paragraph{\textbf{Tron}}
Tron is a two-player simultaneous move game played on a discrete grid, possibly obstructed by
walls~\cite{Samothrakis10Tron,Perick12Comparison,Lanctot13Tron}.
At each step both players move to adjacent cells, and a wall is placed to players' original positions.
A player exits the game if he hits the wall or the opponent.
The goal of both players is to survive as long as possible.
If both players move into a wall, off the board, or into each other at the same turn, the game ends in a draw.
The utility is $+1$ for a win, $0$ for a draw, and $-1$ for a loss.
In the experiments we used an empty grid with no obstacles and various sizes of the grid.

Similarly to pursuit-evasion games, there are many instances of Tron that have pure NE.
However, even if mixed strategies are required, they appear in the middle of the game once both players reach the center of the board and compete over the advantage of possibly being able to occupy more squares.
Once this is determined, the endgame can be solved in pure strategies since it typically consists of filling the available space in an optimal ordering one square at a time.
The following table comparing the number of states demonstrates this characteristics of Tron on a grid $5\times6$:

\vspace{0.1cm}

%\begin{table}[h!]
\begin{center}
\small
%\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%\hline Depth & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13\\
%\hline Pure  & 1 & 4 & 14 & 100 & 565 & 2598 & 9508 & 25964 & 54304 & 83624 & 87009 & 63642 & 23296 & 3127\\
%\hline Mixed & 0 & 0 & 2 & 0 & 9 & 7 & 51 & 92 & 106 & 121 & 74 & 0 & 0 & 0 \\
%\hline
%\end{tabular}
\begin{flushleft}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline Depth & 0 & 1 & 2 & 3 & 4 & 5 & $\ldots$\\
\hline Pure  & 1 & 4 & 14 & 100 & 565 & 2598 & \\
\hline Mixed & 0 & 0 & 2 & 0 & 9 & 7 & \\
\hline
\end{tabular}
\end{flushleft}
\begin{flushright}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline  $\ldots$ & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13\\
\hline  & 9508 & 25964 & 54304 & 83624 & 87009 & 63642 & 23296 & 3127\\
\hline  & 51 & 92 & 106 & 121 & 74 & 0 & 0 & 0 \\
\hline
\end{tabular}
\end{flushright}
\end{center}

\vspace{0.1cm}

The evaluation function is based on how much space is ``owned'' by each player, which is a more accurate version of the space estimation
heuristic~\cite{DenTeuling12Tron} that was used in~\cite{Lanctot13Tron}. A cell is owned by player $i$ if it can be reached
by player $i$ before the opponent. These values are computed using an efficient flood-fill algorithm whose sources start from the two players'
current positions:
\[
eval(s) = \tanh\left(\frac{\textrm{owned}_1 - \textrm{owned}_2}{5}\right).
\]


\subsection{Non-Convergence and Random Tie-Breaking in UCT}\label{sec:exp:brps}

We first revisit the counter-example given in \cite{Shafiei09} showing that
UCT does not converge to an equilibrium strategy in Biased Rock, Paper, Scissors
when using a mixed strategy by normalizing the visit counts.
In our first experiment, we revisit and expand upon on this result, showing the effect of synchronization ocurring when the UCT selection mechanism is fully deterministic (see Subsection~\ref{sec:duct}).

We run MCTS with UCT, Exp3, and Regret Matching variants on Biased Rock, Paper, Scissors
for 100 million ($10^8$) iterations, measuring the exploitability of the strategy recommended by
each variant at regular intervals. The results are shown in Figures \ref{fig:expl-brps1} and \ref{fig:expl-brps2}.

The first observation is that the deterministic UCT does not seem to converge to a low-exploitability strategy (see Figure~\ref{fig:expl-brps1}, top figure). The exploitability of the strategies of
Exp3 and RM variants do converge to low-exploitability strategies (see Figure~\ref{fig:expl-brps2}), and the resulting approximation depends on the amount of exploration.
If less exploration is used, then the resulting strategy is less exploitable, which is natural in the case of a single state. RM does seem to
converge slightly faster than Exp3, as we will see in the remaining domains as well.

\begin{figure}[t!]
\centering
\begin{tabular}{c}
{\small MCTS-UCT (fully deterministic)} \\
\includegraphics[width=0.8\textwidth]{figures/brps-MCTS-UCT.pdf} \\
{\small MCTS-UCT with stochastic tie-breaking} \\
\includegraphics[width=0.8\textwidth]{figures/brps-MCTS-UCT-NONDET.pdf} \\
\end{tabular}
\caption{Exploitability of strategies of recommended by MCTS-UCT over time in Biased Rock, Paper, Scissors. Vertical axis represents exploitability. }
\label{fig:expl-brps1}
\end{figure}

\begin{figure}[t!]
\centering
\begin{tabular}{c}
%{\small MCTS-UCT (fully deterministic)} \\
%\includegraphics[width=0.4\textwidth]{figures/brps-MCTS-UCT.pdf} \\
%{\small MCTS-UCT with stochastic tie-breaking} \\
%\includegraphics[width=0.4\textwidth]{figures/brps-MCTS-UCT-NONDET.pdf} \\
{\small MCTS-Exp3} \\
\includegraphics[width=0.8\textwidth]{figures/brps-MCTS-EXP3.pdf} \\
{\small MCTS-RM} \\
\includegraphics[width=0.8\textwidth]{figures/brps-MCTS-RM.pdf} \\
\end{tabular}
\caption{Exploitability of strategies of recommended by MCTS-Exp3 and MCTS-RM over time in Biased Rock, Paper, Scissors. Vertical axis represents exploitability. }
\label{fig:expl-brps2}
\end{figure}

We then tried adding a stochastic tie-breaking rule to the UCT selection mechanism typically used in MCTS implementations, which chooses an
action randomly when the scores of the best values are ``tied'' (less than $0.01$ apart).
Figure~\ref{fig:expl-brps1}, bottom figure shows the convergence.
One particularly striking observation is that this simple addition leads to a large drop in resulting exploitability, where exploitability
ranges from $[0.5,0.8]$ in the deterministic case, compared to $[0.01,0.05]$ with stochastic tie-breaking. Therefore, stochastic tie-breaking is
enabled in all of our experiments.

In summary, with this randomization UCT appears to be converging to an approximate equilibrium but not to an
exact equilibrium, which is similar to results of UCT in Kuhn poker~\cite{Ponsen11Computing}.

%mlanctot: is this true? %bbosansky: yes, we have run again experiments with randomized uct where deterministic uct had been used before

%\mlanctot{I noticed Vilo added his example to the SM-MCTS section so please modify this paragraph as necessary. Maybe it can be
%removed entirely?} We investigated this further and found a possible
%explanation of this behavior. Consider the matrix game on the right of Figure~\ref{fig:egMatrixGames} in Section~\ref{sec:smg}.
%Suppose DUCT is run on this game and players deterministically prefer the left-action.
%DUCT will always recieve rewards 0 and the exploration bias term will cause the players to round-robin over the actions indefinitely.
%However, each player can than improve by playing first action with probability 1. Breaking ties stochastically will lead the algorithm to
%discover the 1 and -1 rewards with high probability and allow the algorithm the break out or avoid this synchronization trap.

\subsection{Offline Equilibrium Computation}
We now compare the offline performance of the algorithm on all the remaining games.
We measure the overall computation time for each of the algorithms and number of evaluated nodes -- i.e., nodes for which the main method of the backward induction algorithm executed (i.e., nodes evaluated by serialized alpha-beta algorithms are not included in this count, since they may be evaluated repeatedly).
Unless otherwise stated, each data point represents a mean of at least $30$ runs.
% mlanctot: check this %bbosansky: ?

\subsubsection{Goofspiel}
\begin{figure} [t]
\centering
	\begin{subfigure}{0.49\textwidth}
 		\includegraphics[width=1\textwidth]{figures/GS-BT-NF.pdf}\caption{}\label{fig:off:res:gs-bt}
 	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
 		\includegraphics[width=1\textwidth]{figures/GS-BF-NF.pdf}\caption{}\label{fig:off:res:gs-bf}
 	\end{subfigure}
\caption{Running times of exact algorithms on Goofspiel with fixed order of the nature's cards for increasing size of the deck; sub-figure (a) depicts the results with win-tie-loose utilities, (b) depicts the results with point utilities.} \label{fig:off:res:gs}
\end{figure}

We now describe the results for the card game Goofspiel.
First, we analyze the games with a fixed ordering of the cards by nature.

\paragraph{Exact algorithms with fixed ordering}
The results are depicted in Figure~\ref{fig:off:res:gs} (note the logarithmic vertical scale), where the left subfigure depicts the results for win-tie-loose utilities and the right subfigure depicts the results for point utilities.
The comparison on the win-tie-loose variant shows that there is a significant number of subgames with pure Nash equilibrium that can be computed using serialized alpha-beta algorithms.
Therefore, the performance of $\biab$ and $\doab$ is fairly similar and the gap only slowly increases in favor of $\doab$ with the increasing size of the game.
Since serialized alpha-beta is able to solve a large portion of subgames, both of these algorithms significantly reduce the number of the states visited by the backward induction algorithm.
While \textsc{BI} algorithm evaluates than $3.2\times10^7$ nodes in the setting with $7$ cards in more than $2.5$ hours, $\biab$ evaluates only $198,986$ nodes in less than $4$ minutes.
The performance is further improved by $\doab$ that evaluates on average $79,105$ nodes in less than $3$ minutes.
The overheads are slightly higher in case of $\doab$; hence, the time difference between $\doab$ and $\biab$ is relatively small compared to the difference in evaluated nodes.
Finally, the results show that even simple \textsc{DO} algorithm without serialized alpha-beta search can improve the performance of \textsc{BI}.
In the setting with $7$ cards, \textsc{DO} evaluates more than $6\times10^6$ nodes which takes on average almost 30 minutes.

\reviewchange{The results for the point utilities are the same for BI, while DO is slightly worse.
On the other hand, the success of serialized alpha-beta algorithm is significantly lower and it takes both algorithms much more time to solve games of same size.
With $7$ cards, $\biab$ now evaluates more than $2\times10^6$ nodes and it takes the algorithm on average $32$ minutes to find the solution.
$\doab$ is still the fastest and it evaluates more than $3\times10^5$ nodes in less than $13$ minutes on average.}

The performance of algorithms $\biab$ and $\doab$ represent a significant improvement over the results of the pruning algorithm
SMAB presented in \cite{Saffidine12SMAB}.
In their work, the number of evaluated nodes was at best around $29\%$, and the running time improvement was only marginal.

\paragraph{Exact algorithms with chance nodes}

\begin{figure}[t]
\centering
	\begin{subfigure}{0.49\textwidth}
 		\includegraphics[width=1\textwidth]{figures/GS-BT-NN.pdf}\caption{}\label{fig:off:res:gsn-bt}
 	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
 		\includegraphics[width=1\textwidth]{figures/GS-BF-NN.pdf}\caption{}\label{fig:off:res:gsn-bf}
 	\end{subfigure}
\caption{Running times of exact algorithms on Goofspiel with chance nodes for increasing size of the deck; sub-figure (a) depicts the results with win-tie-loose utilities, (b) depicts the results with point utilities.} \label{fig:off:res:gsn}
\end{figure}

Next we compare the exact algorithms in the variant of Goofspiel with standard chance nodes.
Introducing another branching due to moves by nature causes a significant increase in the size of the game tree.
For $7$ cards, the game tree has more than $10^{11}$ nodes, which is $4$ orders of magnitude more than in the case with fixed ordering of nature's cards.
The results depicted in Figure~\ref{fig:off:res:gsn} show that the games become quickly too large to solve exactly and the fastest algorithms solved games with at most $6$ cards.
Relative performance of the algorithms, however, is similar to the case with fixed ordering.
With win-tie-loose utilities, serialized alpha-beta is again able to find pure NE in most of the subgames and prunes out a large fraction of the states.
For the game with $5$ cards, BI evaluates more than $2 \times 10^6$ nodes in almost $10$ minutes, while $\biab$ evaluates only $17,315$ nodes in $27$ seconds and $\doab$ evaluates $6,980$ nodes in $23$ seconds.
As before, serialized alpha-beta algorithm is less helpful in the case with point utilities.
Again with $5$ cards, $\biab$ evaluates $91,419$ nodes in more than $100$ seconds and $\doab$ evaluates $14,536$ nodes in almost $55$ seconds.

\paragraph{Sampling algorithms with fixed ordering}

\begin{figure}[t]
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{figures/convergence-gs-tf.pdf}%\caption{}\label{fig:off:conv:gs-bt}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{figures/convergence-gs-ff.pdf}%\caption{}\label{fig:off:conv:gs-bf}
	\end{subfigure}
\caption{Convergence of sampling algorithms on Goofspiel with $5$ cards and fixed ordering of nature's cards.
Vertical lines correspond to computation times for exact algorithms.
(Top) Goofspiel with win-tie-loose utility values;
(Bottom) Goofspiel with point utilities.} \label{fig:off:conv:gs}
\end{figure}

We now turn to the analysis of the convergence of the sampling algorithms -- i.e., their ability to approximate Nash equilibrium strategies of the complete game.
Figure~\ref{fig:off:conv:gs} depicts the results for Goofspiel game with $5$ cards with fixed ordering cards of nature (note the logarithmic horizontal scale).
We compare MCTS algorithms with three different selection functions (UCT, Exp3, and RM), and OOS.
The results are means out of $30$ runs of each algorithm.
Due to the different selection and update functions, the algorithms differ in the number of iterations per second.
RM is the fastest with more than $2.6\times 10^5$ iterations per second, OOS has around $2\times10^5$ iterations, UCT $1.9\times10^5$, and Exp3 only $5.4\times10^4$ iterations.

The results show that OOS converges the fastest out of all sampling algorithms.
This is especially noticeable in the point-utility settings, where none of the other sampling algorithms were approaching zero error due to the exploration.
MCTS with RM selection function is only slightly slower in the win-tie-loose case, however, other two selection functions perform worse.
While Exp3 eventually converges close to $0$ in the win-tie-loose case, the exploitability of UCT decreases rather slowly and it was still over $0.35$ at the time limit of $500$ seconds.
The best $C$ constant for UCT was 5 in win-tie-loose setting, and $10$ in the point utility setting.
While setting lower constant typically improved slightly the
convergence during the first iterations, the final error was always larger.
%Moreover, when we have not used randomization in UCT, the algorithm was not able to converge to error lower than $0.5$ in the time limit.
%On the other hand, both OOS and RM confirm their quick convergence, witch is aligned with previous results~\cite{Lanctot13Goofspiel}.
The vertical lines represent times for exact algorithms.
In the win-tie-loose case, $\biab$ is slightly faster and finishes first in $0.64$ second, followed by $\doab$ ($0.69$ seconds), \textsc{DO} ($3.1$ seconds), and \textsc{BI} ($6$ seconds).
In the point case, $\doab$ is the fastest ($0.97$ seconds), followed by $\biab$ ($1.3$ seconds), followed by \textsc{DO} and \textsc{BI} with similar times as in the previous case.

\paragraph{Sampling algorithms with chance nodes}

\begin{figure}[t]
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{figures/convergence-gs-tn.pdf}%\caption{}\label{fig:off:conv:gs-bt}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{figures/convergence-gs-fn.pdf}%\caption{}\label{fig:off:conv:gs-bf}
	\end{subfigure}
\caption{Convergence of sampling algorithms on Goofspiel with $4$ cards and chance nodes.
Vertical lines correspond to computation times of exact algorithms.
(Top) Goofspiel with win-tie-loose utility values;
(Bottom) Goofspiel with point utilities.} \label{fig:off:conv:gsn}
\end{figure}

We also performed the experiments in the setting with chance nodes.
Due to the size of the game tree, we have reduced the number of cards to $4$, since the size of this game tree is comparable to the case with $5$ cards and fixed ordering of cards.
The results depicted in Figure~\ref{fig:off:conv:gsn} show a similar behavior of the sampling algorithms as we have observed in the previous case.
OOS converges the fastest, followed by RM, and EXP3.
Main difference is in the convergence of UCT, however, this is mostly due to the fact that pure NE exists in Goofspiel with 4 cards; hence, UCT can better identify the best action to play and converges more quickly to less exploitable strategy than in the case with 5 cards.
Surprisingly, the convergence of the algorithms does not change that dramatically with the introduction of point utilities as in the previous case.
The main reason is that the range of the utility values is smaller compared to the previous case (there is one card less in the present setting and the missing cards has the highest value).
For comparison, we again use the vertical lines to denote times of exact algorithms.
$\biab$ and $\doab$ are almost identically fast, with $\doab$ being slightly faster, followed by \textsc{DO} and \textsc{BI}.


\subsubsection{Pursuit-Evasion Games}
\begin{figure}
\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/PEG4x4.pdf}\caption{}\label{fig:off:res:peg4}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/PEG5x5.pdf}\caption{}\label{fig:off:res:peg5}
	\end{subfigure}
\caption{Running times of exact algorithms on pursuit-evasion game with increasing number of moves: sub-figure (a) depicts the results on $4\times4$ grid graph, (b) depicts results for $5\times5$ grid.} \label{fig:off:res:peg}
\end{figure}

The results on pursuit-evasion games show more significant improvement when comparing $\doab$ and $\biab$ (see Figure~\ref{fig:off:res:peg}). In all settings, the $\doab$ is significantly the fastest. When we compare the performance on graph $5\times5$ with depth set to $6$, \textsc{BI} evaluates more than $4.9\times10^7$ nodes that takes more than $13$ hours. On the other hand, $\biab$ evaluates on average $42001$ nodes taking almost $10$ minutes ($584$ seconds). Interestingly, the benefits of pure integration with alpha-beta search is not that helpful in this game.
This is apparent from results of \textsc{DO} algorithm that evaluates less than $2\times10^6$ nodes but it takes slightly over $9$ minutes on average ($547$ seconds). Finally, $\doab$ evaluates only $6692$ nodes and it takes the algorithm less than $3$ minutes.

Large parts of these pursuit-evasion games can be solved by serialized alpha-beta algorithms.
These parts typically corresponds to clearly winning, or clearly losing positions for a player; hence, serialized alpha-beta algorithms are able to prune substantial portion of the space.
However, since there are only two pursuit units, it is still necessary to use mixed strategies for final coordination (capturing the evader close to edge of the graph), and thus mixing strategy occurs near the end of the game tree.
Therefore, serialized alpha-beta is not able to solve all subgames, while double-oracle provides additional pruning since many of the actions in the subgames are leading to the same outcome and not all of them are required to find equilibrium strategies.
This causes additional improvement in computation time for $\doab$ compared to $\biab$ and all the other algorithms.%\vlisy{This argument is not very consistent with $\biab$ evaluating $\tfrac{1}{1000}$ of the nodes BI evaluates, while DO evaluating $\tfrac{1}{20}$. The reason is likely the cost of serialized searches.}

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{figures/convergence-peg.pdf}
\caption{Convergence of sampling algorithms on pursuit-evasion game, on $4\times4$ graph, with depth set to $4$. The vertical lines correspond to computation times of exact algorithms.} \label{fig:off:conv:peg}
\end{figure}

We turn to convergence of the sampling algorithms.
In terms of number of iterations per second, again RM was the fastest and OOS the second fastest with similar performance as in Goofspiel.
UCT achieved slightly less ($1.7\times10^5$ iterations per second), and Exp3 only $2.6\times10^4$ iterations.
The results are depicted in Figure~\ref{fig:off:conv:peg} for the smaller $4\times4$ graph and $4$ moves for each player (note again the logarithmic horizontal scale).
The starting positions were selected such that there does not exist a pure NE strategy in the game.
The results again show that OOS is overall the fastest out of all sampling algorithms.
During the first iterations, RM preform similarly, however, OOS is able to keep the convergence rate, RM converges more slowly.
UCT again converges to an exploitable strategy with error $1.16$ at best in the time limit of $500$ seconds ($C=2$).
Finally, Exp3 converges even more slowly compared to Goofspiel.
The main difference between the games is the size of the branching factor for the second player (pursuer controls two simultaneously moving units), which can cause more difficulties for the sampling algorithms to estimate good strategies.

As before, the vertical lines represent times for exact algorithms.
In pursuit-evasion game of this setting, $\doab$ is slightly faster and finishes first in $2.77$ seconds, following by $\biab$ ($2.89$ seconds), \textsc{DO} ($5.48$ seconds), and \textsc{BI} ($12.5$).

\subsubsection{Oshi-Zumo}
\begin{figure}
\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/OZ-K4.pdf}\caption{}\label{fig:off:res:oz4}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/OZ-K4-BF.pdf}\caption{}\label{fig:off:res:oz4-bf}
	\end{subfigure}
\caption{Running times of exact algorithms on Oshi-Zumo with $K$ set to $4$ and increasing number of coins: sub-figure (a) depicts the results for binary utilities, (b) depicts the results with point utilities.} \label{fig:off:res:oz}
\end{figure}

Many instances of the Oshi-Zumo game have Nash equilibria in pure strategies regardless on the type of utility function.
Although this does not hold for all the instances, the size of the subgames with pure NE are rather large and cause dramatic computation speed-up for both algorithms using serialized alpha-beta search.
If the game does not have equilibria in pure strategies, the mixed strategies are still required only near the root node and large end-games are solved using alpha-beta search.
Note that this is different to pursuit-evasion games, where mixed strategies were necessary close to the end of the game tree.
Figure~\ref{fig:off:res:oz} depicts the results with the parameter $K$ set to $4$ and for two different settings of utility function\footnote{We have also performed the same experiments with $K$ set to $3$, but the conclusions were the same as in case $K=4$.}; either a win-tie-loose utilities (left sub-figure) or point utilities (right sub-figure).
In both cases, the graphs show the breaking point when the game stops having an equilibrium in pure strategies ($15$ coins and more for each player).
The advantage of $\biab$ and $\doab$ algorithms that exploit serialized variants of alpha-beta algorithms is dramatic.
We can see that both \textsc{BI} and \textsc{DO} scale rather badly.
The algorithms were able to scale up to $13$ coins in reasonable time.
For setting with $K=4$ and $13$ coins, it takes almost $2$ hours for \textsc{BI} to solve the game (the algorithm evaluates $1.5\times10^7$ nodes) regardless of the utility values.
\textsc{DO} improves the performance slightly (the algorithm evaluates $2.8\times10^6$ nodes in $17$ minutes for the win-tie-loose utilities; the performance is slightly worse for point utilities: $5\times10^6$ nodes in $23$ minutes).
Both $\biab$ and $\doab$, however, solved a single alpha-beta search on each serialization finding a pure NE.
Therefore, their performance is identical and it takes around $1.5$ seconds to solve the game for both types of utilities.
Although with an increasing number of coins the algorithms $\biab$ and $\doab$ need to find mixed Nash equilibria, their performance is very similar for both types of utilities.
As expected, the case with point utilities is more challenging and the algorithms scale worse -- for $18$ coins both algorithms solve the game with win-tie-loose utilities in approximately $1$ hour ($\biab$ in $50$ minutes, $\doab$ in $64$), it takes the algorithms around $3$ hours to solve the case with point utilities ($\biab$ in $191$ minutes, $\doab$ in $172$ minutes).

\begin{figure}[t]
\centering
	\begin{subfigure}{0.85\textwidth}
		\includegraphics[width=1\textwidth]{figures/convergence-oz.pdf}
	\end{subfigure}
	\begin{subfigure}{0.85\textwidth}
		\includegraphics[width=1\textwidth]{figures/convergence-oz-bf.pdf}
	\end{subfigure}
\caption{Convergence of sampling algorithms on Oshi-Zumo game, with $10$ coins, $K=3$, and $M=1$. The vertical lines correspond to computation times of exact algorithms.
(Top) Oshi-Zumo with win-tie-loose utility values;
(Bottom) Oshi-Zumo with point utilities.} \label{fig:off:conv:oz}
\end{figure}

\reviewchange{
Turning to the sampling algorithms reveals that the game is difficult to approximate even in the win-tie-lose setting.
Figure~\ref{fig:off:conv:oz} depicts results for convergence of the sampling algorithms for the game with $10$ coins, $K$ set to $3$ and minimum bid set to $1$. This is an easy game for $\doab$ and $\biab$ with pure strategy and both of these algorithms are able to solve the game in less than a second ($0.73$). However, due to large branching factor for both players ($10$ actions at the root node for each player) all sampling algorithms converge extremely slowly. The performance of the algorithms in terms of iterations per second is similar to the previous games, however, OOS is slightly better in this case with $1.9\times10^5$ iterations per second compared to the second RM with $1.6\times10^5$ iterations per second.
}

As before, OOS is the best converging algorithm, however, in a given time limit ($500$ seconds) the reached error was only slightly below $0.3$ ($0.29$). On the other hand, all of the other sampling algorithms perform significantly worse -- RM ends with error slightly over $1$, UCT ($C=2$) with $1.50$, and Exp3 with $1.88$.
This confirms our findings from the previous experiment that increasing branching factor slows down the convergence rate.
Secondly, since there is a pure Nash equilibrium in this particular game configuration, the convergence of the algorithms is also slower since they essentially mix the strategy during the iterations in order to explore the unvisited parts of the game tree. Since none of the sampling algorithms can directly exploit this fact, their performance in offline solving games like Oshi-Zumo is not compelling. On the other hand, the existence of pure NE explains the better performance of UCT compared to Exp3 that is forced to explore more broadly.
\reviewchange{Moreover, convergence takes even more time in the point utility case, since the range of the utility values is larger.
OOS is again the fastest and converges to error $0.45$ within the time limit, RM to $1.41$, UCT ($C=4$) to $3.1$, and Exp3 $3.7$.}


\subsubsection{Random Games}
\begin{figure}[t]
\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/RG-BF4-BIN-FALSE.pdf}\caption{}\label{fig:off:res:rgbf4}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/RG-BF5-BIN-FALSE.pdf}\caption{}\label{fig:off:res:rgbf5}
	\end{subfigure}
\caption{Running times of exact algorithms on randomly generated games with increasing depth: sub-figure (a) depicts the results with branching factor set to $4$ actions for each player, (b) depicts the results with branching factor $5$.} \label{fig:off:res:rg}
\end{figure}

In the first variant of the randomly generated games we used games with utility values randomly drawn from a uniform distribution $[0,1]$.
Such games represent an extreme case, where neither alpha-beta search, nor double-oracle algorithm can save much computation time, since each action can lead to arbitrarily good or bad terminal state.
In these games, \textsc{BI} algorithm is typically the fastest.
Even though both $\biab$ and $\doab$ evaluate marginally less nodes ($\approx90\%$), the overhead of the algorithms (repeated calculation of alpha-beta algorithm, repeatedly solving linear programs, etc.) causes slower runtime performance in this case.

However, completely random games are rarely instances that need to be solved in practice.
The situation changes, when we use the intuition of good and bad moves and thus add correlation to the utility values.
Figure~\ref{fig:off:res:rg} depicts the results for two different branching factors $4$ and $5$ for each player and increasing depth.
The results show that $\doab$ outperforms all remaining algorithms, although the difference is rather small (still statistically significant).
On the other hand, \textsc{DO} without serialized alpha-beta is not able to outperform \textsc{BI}.
This is most likely caused by a larger support in mixed subgame equilibria that cause enumerating most of the actions by the double-oracle algorithm.
Moreover, this is also demonstrated by the performance of $\biab$ that is only slightly better compared to \textsc{BI}.

The fact that serialized alpha-beta is less successful in randomly generated games is noticeable also when comparing the number of evaluated nodes.
For case with branching factor set to $4$ for both players, and depth $7$, \textsc{BI} evaluates almost $1.8\times10^7$ nodes in almost $3.5$ hours, while $\biab$ evaluates more than $\approx1\times10^7$ nodes in almost $3$ hours.
\textsc{DO} evaluates even more nodes compared to $\biab$ ($\approx1.2\times10^7$) and it is slower compared to both \textsc{BI} and $\biab$.
Finally, $\doab$ evaluates $\approx2\times10^6$ nodes on average and it takes the algorithm slightly over $80$ minutes.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/convergence-rg.pdf}
\caption{Convergence of sampling algorithms on random game with branching factor $4$ and depth $5$. The vertical lines correspond to computation times of exact algorithms.} \label{fig:off:conv:rg}
\end{figure}

Figure~\ref{fig:off:conv:rg} depicts the results for convergence of the sampling algorithms for the random game with correlated utility values, branching factor set to $4$ and depth $5$.
The number of iterations per second is similar to Goofspiel, with Exp3 being the exception able to achieve more than $6.5\times10^4$ iterations per second, which is still significantly the lowest number of iterations.
Interestingly, there is a much less difference between the performance of the sampling algorithms in this game.
Since these games are generally more mixed (i.e., NE require to use mixed strategies in many states of the game), they are much more suitable for the sampling algorithms.
OOS can be considered a winner in this setting, however, the performance of RM is very similar.
Again, since the game is more mixed, Exp3 outperforms UCT in the longer run.
Exploration constant for UCT was set to 12 due to larger utility variance in this setting.

\subsubsection{Tron}

\begin{figure}
\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/Tron-1.pdf}\caption{}\label{fig:off:res:tron1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=1\textwidth]{figures/Tron-2.pdf}\caption{}\label{fig:off:res:tron2}
	\end{subfigure}
\caption{Running times of exact algorithms on Tron with increasing $width$ of the graph: sub-figure (a) depicts the results with $height$ of the graph set to $width - 1$, (b) depicts the results with $height = width$.} \label{fig:off:res:tron}
\end{figure}

Performance of exact algorithms in Tron is affected by the fact that pure NE exist in all smaller instances (the results are depicted for two different ratios of dimensions of the graph in Figure~\ref{fig:off:res:tron}).
Therefore, $\biab$ and $\doab$ are essentially the same since serialized alpha-beta is able to solve the game.
Moreover, the performance of standard BI is very weak, since the size of the game increases dramatically with increasing size of the grid (the longest branch of the game tree has $\left(0.5\cdot w\cdot l - 1\right)$ joint actions, where $w$ and $l$ are dimensions of the grid).
While BI is able to solve the grid $5\times6$ in $96$ seconds, it takes around $30$ minutes to solve $6\times6$ grid.
By comparison, DO solves the $6\times6$ instance in $235$ seconds, and both $\biab$ and $\doab$ in $0.6$ seconds.
The largest graph the algorithms $\biab$ and $\doab$ solved had size $9\times8$ taking almost $5$ hours to solve.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/convergence-tron.pdf}
\caption{Convergence comparison of different sampling algorithms on Tron on grid $5\times6$. The vertical lines correspond to computation times for exact algorithms.} \label{fig:off:conv:tron}
\end{figure}

The size of the game tree in Tron also causes slow convergence for sampling algorithms.
This is apparent also in the number of iterations that is slightly slower for the algorithms.
OOS is the fastest performing $1.3\times 10^5$ iterations per second, RM achieves $1.2\times 10^5$ iterations, UCT only $8\times10^4$, and Exp3 is again the slowest with $7.8\times10^4$ iterations per second.
Figure~\ref{fig:off:conv:tron} depicts the results for the grid $5\times6$.
Consistently with the previous results, OOS performs the best and it is able to converge to very close to exact solution in $300$ seconds.
Similarly, both RM and Exp3 are again eventually able to converge to a very small error, however, it take them more time and in the time limit they achieve error $0.05$, or $0.02$ respectively.
Finally, UCT ($C=5$) performs reasonably good during the first $10$ seconds, where the exploitability is better than both RM and Exp3.
This is most likely due to existence pure NE, however, the length of the game tree prohibits UCT to converge and the best error the algorithm was able to achieve in the time limit was equal to $0.68$.

\subsubsection{Summary}

The offline comparison of the algorithms offers several conclusions.
Among the exact algorithms, $\doab$ is clearly the best algorithm, since it typically outperforms all other algorithms (especially in pursuit-evasion games and random games). Although for smaller games (e.g., Goofspiel with $5$ cards)  $\biab$ can be slightly faster, this difference is not significant and $\doab$ is never significantly slower compared to $\biab$.

Among the sampling algorithms, OOS is the clear winner since it is often able to quickly converge to a very small error and significantly outperforms all variants of MCTS.
On the other hand, comparing OOS and $\doab$, the exact $\doab$ algorithm is always faster and it is able to find an exact solution much faster compared to OOS.
Moreover, $\doab$ has significantly lower memory requirements since it is a depth-first search algorithm and does not use any form of global cache, while OOS iteratively constructs the game tree in memory.


\subsection{Online Search}

We now compare the performance of the algorithms in head-to-head matches in large instances of the same games as we used in the offline equilibrium computation experiments. Each algorithm has a strictly limited computation time per move set to one or five seconds. After this time, it has to output an action to be played in the current game state and afterwards, it is informed about the action selected by its opponent. The game proceeds to the following state and the algorithms have additional time for computation. As described in Section~\ref{sec:online}, all algorithms keep results of previous computations and do not start form scratch in the following move. Since the results with one and five seconds of computation are practically the same, we presents the results with one second in details and only comment on the five second results later.

We compare all of the approximative sampling algorithms and $\doab$ as a representative of backward induction algorithms, because it was clearly the fastest algorithm in all of the considered games.
Finally, we also included random player (denoted RAND) into the tournament to confirm that the algorithms choose better strategies than the simple random game play.
%In the following experiments, we aim to identify the algorithms that are most suitable in the online search setting and study the relation of the performance in approximating the equilibrium in the offline setting and actual game playing performance in the matches.
We report expected rewards and win rates of the algorithms, in which a tie is counted as half win.
The parameters of the algorithms are tuned for each domain separately.
We first present the comparison of different algorithms and we discuss the influence of the parameters in Subsection~\ref{sec:eval:online:tuning}.

\reviewchange{In this subsection, we show cross tables of each algorithm (on the row) matched up against each competitor algorithm (on the column).}
Each entry represents a mean of at least $1000$ matches with the half of the width of the $95\%$ confidence interval in brackets, \eg $52.9(0.3)$ refers to $52.9\% \pm 0.3\%$.
\reviewchange{The result shown is the win rate for the row player, so as an example in the standard game of Goofspiel (top of Table~\ref{fig:matches:goof}) $\doab$ wins $67.2 \% \pm 1.4 \%$ of games against the random player.}

\subsubsection{Goofspiel}

\begin{table}
\centering
\begin{scriptsize}

Goofspiel: 13 cards, unknown point card sequence, win rate
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.2)&UCT(0.6)&EXP3(0.3)&RM(0.1)&RAND\\\hline
$\doab$&&51.6(3.2)&60.1(3.0)&54.0(3.4)&58.7(4.2)&67.2(1.4)\\
OOS&48.4(3.2)&&51.2(2.1)&52.5(2.2)&47.9(3.0)&81.4(1.7)\\
UCT&39.9(3.0)&48.8(2.1)&&55.6(2.1)&46.6(4.7)&77.3(1.8)\\
EXP3&46.0(3.4)&47.5(2.2)&44.4(2.1)&&41.0(3.0)&86.1(1.5)\\
RM&41.3(4.2)&52.1(3.0)&53.4(4.7)&59.0(3.0)&&84.0(1.1)\\
RAND&32.8(1.4)&18.6(1.7)&22.7(1.8)&13.9(1.5)&16.0(1.1)&\\
\hline
\end{tabular}

Goofspiel: 13 cards, known point card sequence, win rate
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS&UCT&EXP3&RM&RAND\\\hline
$\doab$&&30.8(2.7)&39.3(3.0)&33.8(2.9)&32.7(2.7)&67.2(2.9)\\
OOS&\textbf{69.2(2.7)}&&46.2(3.0)&51.8(3.0)&\textbf{49.6(3.0)}&83.8(2.3)\\
UCT&60.7(3.0)&\textbf{53.8(3.0)}&&\textbf{57.1(2.9)}&48.6(2.9)&79.5(2.5)\\
EXP3&66.2(2.9)&48.2(3.0)&42.9(2.9)&&46.5(3.0)&\textbf{85.8(2.1)}\\
RM&67.3(2.7)&50.4(3.0)&\textbf{51.4(2.9)}&53.5(3.0)&&84.2(2.2)\\
RAND&32.8(2.9)&16.2(2.3)&20.5(2.5)&14.2(2.1)&15.8(2.2)&\\
\hline
\end{tabular}

Goofspiel: 13 cards, known point card sequence, expected reward
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.3)&UCT(0.8)&EXP3(0.2)&RM(0.1)&RAND\\\hline
$\doab$&&-7.59(0.97)&-1.80(0.91)&-7.30(0.87)&-5.89(1.01)&6.67(0.99)\\
OOS&7.59(0.97)&&1.19(0.78)&0.30(0.78)&0.35(0.76)&14.42(0.96)\\
UCT&1.80(0.91)&-1.19(0.78)&&-0.77(0.76)&-1.94(0.73)&13.30(1.00)\\
EXP3&7.30(0.87)&-0.30(0.78)&0.77(0.76)&&-1.65(0.72)&15.89(1.00)\\
RM&5.89(1.01)&-0.35(0.76)&1.94(0.73)&1.65(0.72)&&14.20(0.98)\\
RAND&-6.67(0.99)&-14.42(0.96)&-13.30(1.00)&-15.89(1.00)&-14.20(0.98)&\\
\hline
\end{tabular}
\end{scriptsize}
\caption{Results of head-to-head matches in Goofspiel variants with exploration parameter settings indicated in the header.}\label{fig:matches:goof}
\end{table}

In head-to-head comparison, we use the standard Goofspiel with 13 cards. Additionally, for the sake of consistency with the offline results, we also evaluate the variant with a fixed known sequence of the point cards, removing all chance nodes. The full game has more than $2.4\times 10^{29}$ leaf nodes and the variant with fixed point card sequence has still more than $3.8\times 10^{19}$ leaf nodes. The results are presented in Table~\ref{fig:matches:goof}, where the top table shows the win rates of the algorithms in the full game and the other two tables the win rates and expected number of points gained by the algorithms in the game with a fixed point card sequence. The results for the know card sequence are the mean of $10$ fixed random orderings. For each table, the algorithms were set up to optimize the presented measure (\ie, win rate or expected points) and the exploration parameters were tuned to the values presented in the header of the table.

First, we can see that finding a good strategy in Goofspiel is difficult for all the algorithms.
This is noticeable thanks to the results of RAND player, that performs reasonably well
(RAND typically loses almost every match in all the remaining game domains).
Next, we analyze the results of the $\doab$ algorithms compared to the sampling algorithms.
The larger game variant seems to be too large and noisy for the sampling algorithms to converge to good strategies.
$\doab$ with a domain-specific heuristic evaluation function outperforms all sampling algorithms.
With the exception of OOS, the performance difference is statistically significant.
In the game variants with the fixed point card sequence, the situation is the opposite.
$\doab$ does not win significantly against any of the sampling algorithms.
The sampling algorithms can already find better state evaluations than the hand-coded heuristic evaluation function in $\doab$.
Also, the limited overhead due to short and fixed length of this game could favor sampling methods compared to the
matrix constructions in $\doab$.

The differences in the performance of sampling algorithms are relatively small.
In the variant with unknown card sequence, Exp3 performs worst, outperforming only the random player.
OOS significantly outperforms only Exp3 and RM is the best among the sampling algorithms, winning over all other algorithms and with the exception of OOS significantly.
In the game with fixed card sequence, RM still wins most matches against any of the opponents, but UCT manages to better exploit the weaker opponents and win more often than RM against EXP3 and OOS.
When optimizing the expected points OOS wins slightly against all sampling algorithms, but with the exception of UCT, it is not statistically significant. RM looses against OOS slightly, wins significantly against UCT and EXP3.

In summary, RM is the only algorithm that did not loose significantly against any other sampling algorithm in any other game variant and often won significantly. Exp3 was weakest overall and the performance of UCT was very unstable along game variants and various opponents.

\vlisy{stress in conclusions that RM is most stable in choice of the parameter, which is an important advantage}

\subsubsection{Oshi-Zumo}

\begin{table}[t!]
\centering
\begin{scriptsize}

Oshi Zumo: 50 coins, 2$\cdot$3 + 1 positions, win rate
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.2)&UCT(0.4)&EXP3(0.8)&RM(0.1)&RAND\\\hline
$\doab$&&\textbf{79.5(1.7)}&\textbf{76.8(2.6)}&74.0(2.7)&\textbf{81.0(2.4)}&98.8(0.5)\\
OOS&20.5(1.7)&&27.7(2.6)&57.1(2.1)&51.2(2.1)&98.9(0.4)\\
UCT&23.1(2.6)&72.3(2.6)&&\textbf{83.0(2.0)}&70.3(2.6)&\textbf{99.9(0.2)}\\
EXP3&\textbf{26.1(2.7)}&42.9(2.1)&17.0(2.0)&&44.5(2.8)&98.5(0.5)\\
RM&18.9(2.4)&48.8(2.1)&29.6(2.6)&55.5(2.8)&&99.0(0.4)\\
RAND&1.2(0.5)&1.1(0.4)&0.1(0.2)&1.5(0.5)&1.0(0.4)&\\
\hline
\end{tabular}

Oshi Zumo: 50 coins, 2$\cdot$3 + 1 positions, expected reward
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.2)&UCT(0.4)&EXP3(0.8)&RM(0.1)&RAND\\\hline
$\doab$&&\textbf{2.51(0.18)}&\textbf{1.84(0.21)}&\textbf{1.08(0.22)}&\textbf{2.75(0.17)}&3.65(0.09)\\
OOS&-2.51(0.18)&&-0.53(0.19)&-0.57(0.16)&0.25(0.20)&3.87(0.05)\\
UCT&-1.84(0.21)&0.53(0.19)&&0.19(0.15)&0.58(0.17)&\textbf{3.93(0.02)}\\
EXP3&\textbf{-1.08(0.22)}&0.57(0.16)&-0.19(0.15)&&0.53(0.16)&3.86(0.04)\\
RM&-2.75(0.17)&-0.25(0.20)&-0.58(0.17)&-0.53(0.16)&&3.87(0.04)\\
RAND&-3.65(0.09)&-3.87(0.05)&-3.93(0.02)&-3.86(0.04)&-3.87(0.04)&\\
\hline
\end{tabular}

Oshi Zumo: 50 coins, 2$\cdot$3 + 1 positions, win rate, evaluation function
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.3)&UCT(0.8)&EXP3(0.8)&RM(0.1)&RAND\\\hline
$\doab$&&61.8(3.0)&8.3(1.5)&48.6(3.1)&58.6(3.1)&98.8(0.7)\\
OOS&38.2(3.0)&&23.1(2.6)&33.6(2.9)&44.1(3.0)&99.6(0.4)\\
UCT&\textbf{89.5(1.8)}&\textbf{73.6(2.7)}&&\textbf{83.2(2.3)}&\textbf{70.5(2.8)}&\textbf{99.8(0.3)}\\
EXP3&48.5(3.1)&66.7(2.9)&22.3(2.5)&&57.9(3.0)&98.7(0.7)\\
RM&37.8(3.0)&56.9(3.0)&\textbf{28.4(2.8)}&40.5(2.9)&&99.4(0.5)\\
RAND&1.5(0.8)&0.4(0.4)&0.2(0.2)&1.3(0.7)&0.2(0.3)&\\
\hline
\end{tabular}
\end{scriptsize}
\caption{Results of head-to-head matches in Oshi-Zumo variants with exploration parameter settings indicated in the header. In the first two tables only $\doab$ uses evaluation function and in the third table all algorithms use the evaluation function.}\label{fig:matches:oz}
\end{table}

In Oshi-Zumo, we use the setting with 50 coins, K=3 fields on each side of the board and minimal bet of 1.
%We chose this parameters to be consistent with \cite{XXX}\bbosansky{Missing reference} and to have a game with larger branching factor.
The size of the game is large with strictly more than $10^{15}$ leaves (number of choices of a single player in the game; hence, square of this number is the upper bound on the number of leaves) and 50 actions for each player in the root.

In this game, the evaluation function used by $\doab$ is much stronger than in Goofspiel and all the sampling algorithms without domain specific knowledge perform poorly against this algorithm (see Table~\ref{fig:matches:oz}). The loss is even higher with 5 seconds per move. None of the sampling algorithms perform significantly better than the others.

In the offline experiment (Figure~\ref{fig:off:conv:oz}), none of the sampling algorithms was able to converge anywhere close to the equilibrium in short time. Moreover, the game used in the offline experiments was many orders of magnitude smaller (there were $10$ coins for each player).
In spite of the negative results in the offline experiments, all sampling algorithms are able to find reasonably good strategy.
UCT is clearly the strongest sampling algorithm in all variants.
In win rate, its strongest opponent among the sampling algorithms is RM (UCT wins only 70.3\% of games). Exp3 is the weakest algorithm in optimizing the win rate, but it looses only to UCT in optimizing expected reward. RM is the weakest in that setting.

Because of high quality of the evaluation function in this game, we performed experiments where the rollout simulation was replaced with the evaluation function.
The the results are presented in the third table of Table~\ref{fig:matches:oz}).
The quality of play of all sampling algorithms is significantly improved when using the evaluation function.
However, $\doab$ still wins over OOS and RM in both time settings.
UCT is clearly the best and OOS the weakest.

The reason why UCT performs well in this game is that the game mostly requires pure strategies, rather then precise mixing between multiple strategies (see Section~\ref{sec:eval:domains}). UCT is able to quickly disregard other actions, if a single action is optimal.

\subsubsection{Random Games}

\begin{table}
\centering
\begin{scriptsize}

\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.1)&UCT(1.5)&EXP3(0.6)&RM(0.3)&RAND\\\hline
$\doab$&&55.4(3.3)&43.7(2.8)&49.2(2.8)&48.1(2.8)&88.8(1.8)\\
OOS&45.6(3.3)&&33.5(2.5)&43.5(2.7)&42.5(2.8)&85.0(2.4)\\
UCT&\textbf{56.3(2.8)}&\textbf{66.5(2.5)}&&\textbf{67.4(2.5)}&\textbf{55.7(2.6)}&95.9(1.2)\\
EXP3&50.8(2.8)&56.5(2.7)&32.6(2.5)&&42.9(2.7)&\textbf{96.0(1.1)}\\
RM&51.9(2.8)&57.5(2.8)&\textbf{44.3(2.6)}&57.1(2.7)&&93.1(1.5)\\
RAND&11.2(1.8)&15.0(2.4)&4.1(1.2)&4.0(1.1)&6.9(1.5)&\\
\hline
\end{tabular}


\end{scriptsize}
\caption{Win-rate in head-to-head matches of Random games (15,5).}\label{fig:matches:rand}
\end{table}


The next set of matches was played on 10 different random games with each player having 5 actions in each stage and depth 15. Hence, the game has more than $9.3\times 10^{20}$ leaf nodes. In order to compute the win-rates as in other games, we use signum of the utility value defined in Subsection~\ref{sec:eval:domains}. The results are presented in Table~\ref{fig:matches:rand}.

The clearly best performing algorithm in this domain is UCT.
RM is the second, losing only to UCT.
OOS has the weakest performance in spite of good convergence results in the offline settings (see~Figure~\ref{fig:off:conv:rg}). The reason is the quickly growing variance and decreasing number of samples in longer games, which we discuss in more details in Section~\ref{???}. \mlanctot{TODO: Fix this reference, are we adding a section for this? I'm happy to talk about it but could use a few numbers.}
%We also performed an experiment with larger branching factor (15) and smaller depth (5), but the results were very similar to this setting.\vlisy{I assume the new OOS would perform much better there!}

\subsubsection{Tron}

\begin{table}[t!]
\centering
\begin{scriptsize}
Tron: 13x13 grid, win rate
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.1)&UCT(0.6)&EXP3(0.5)&RM(0.1)&RAND\\\hline
$\doab$&&\textbf{79.3(2.0})&\textbf{55.6(2.0)}&\textbf{66.7(2.3)}&\textbf{62.6(2.2)}&\textbf{98.6(0.5)}\\
OOS&20.7(2.0)&&29.4(2.2)&46.1(1.8)&38.0(2.2)&97.2(0.5)\\
UCT&44.4(2.0)&70.6(2.2)&&64.8(2.2)&57.0(2.1)&98.0(0.6)\\
EXP3&33.3(2.3)&53.9(1.8)&35.1(2.2)&&44.3(2.3)&97.7(0.5)\\
RM&37.4(2.2)&62.0(2.2)&43.0(2.1)&55.7(2.3)&&97.7(0.7)\\
RAND&1.4(0.5)&2.9(0.5)&2.0(0.6)&2.3(0.5)&2.3(0.7)&\\
\hline
\end{tabular}

Tron: 13x13 grid, win rate, evaluation function
\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.1)&UCT(2)&EXP3(0.1)&RM(0.2)&RAND\\\hline
$\doab$&&\textbf{51.5(2.0)}&45.6(1.8)&\textbf{58.0(2.0)}&48.9(2.1)&98.9(0.5)\\
OOS&49.4(1.6)&&52.6(1.3)&54.4(1.2)&\textbf{53.1(1.2)}&97.8(0.7)\\
UCT&\textbf{65.3(2.1)}&46.5(1.3)&&49.8(0.7)&46.5(0.9)&\textbf{99.0(0.4)}\\
EXP3&46.6(2.1)&45.2(1.2)&50.4(0.7)&&45.5(1.0)&98.2(0.6)\\
RM&56.7(2.2)&48.5(1.2)&\textbf{53.2(0.9})&53.9(0.9)&&98.8(0.5)\\
RAND&2.3(0.7)&2.0(0.6)&1.5(0.6)&1.9(0.6)&1.7(0.6)&\\
\hline
\end{tabular}
\end{scriptsize}
\caption{Win-rate in head-to-head matches of Tron with random simulations (top) and evaluation function in the sampling algorithms (bottom).}\label{fig:matches:tron}
\end{table}

The large variant of Tron in our evaluation was played on en empty $13\times 13$ board. The branching factor of this game is up to 4 for each player and its depth is up to 83 moves. This variant of Tron has more than $10^{21}$ leaves in the game tree\footnote{The number only estimates the number of possible paths when both players stay on their half of the board.}.
The results are shown in Table~\ref{fig:matches:tron}.

The evaluation function in Tron quite well approximates the actual situation in the game; hence, $\doab$ strongly outperforms all other algorithms when they do not use the evaluation function (top). Its win-rates are even higher with more time per move.
UCT is its strongest opponent, which loses 55.6\% matches and wins over all other sampling algorithms in mutual matches. This is again because of the low need for mixed strategies in this game.
OOS performs the worst because of the high length of the game. It won only 20.7\% matches against $\doab$ and 29.4\% matches against UCT.
\reviewchange{This is consistent with previous analyses in Tron where the best-performing algorithms, including the winner of the 2011 Google AI Challenge,
were based on depth-limited minimax searches~\cite{DenTeuling12Tron,Perick12Comparison}.}

As in the case of Oshi Zumo, we also run the matches with the evaluation function to replace the random rollout simulation in the sampling algorithms.
The usage of the evaluation function improves the performance of all sampling algorithms.
The difference is most notable for OOS, since using the evaluation function strongly reduces the length of the game.
When sampling algorithms use the evaluation function, $\doab$ is already outperformed by UCT and RM.
Exp3 is the weakest algorithm, but the results among the sampling algorithms are not transitive.
OOS significantly outperforms RM, RM outperforms UCT, UCT outperforms $\doab$, and $\doab$ outperforms OOS. This means that none of the algorithms plays close to the equilibrium and each can be exploited by a well chosen opponent.

\subsubsection{Pursuit-Evasion Game}

\begin{table}
\centering
\begin{scriptsize}

\begin{tabular}{|r|cccccc|}\hline
&$\doab$&OOS(0.3)&UCT(0.8)&EXP3(0.5)&RM(0.1)&RAND\\\hline
$\doab$& \textbf{80.9(2.4)}& 91.3(1.7)& \textbf{63.2(3.0)}& 86.3(2.1)& 78.4(2.6)& 99.9(0.2)\\
OOS(0.2)& 76.3(2.6)& 91.2(1.8)& 57.8(3.1)& 85.8(2.2)& 79.3(2.5)& 99.8(0.3)\\
UCT(1.5)& 75.1(2.7)& \textbf{94.2(1.4)}& 57.6(3.1)& \textbf{88.9(1.9)}& \textbf{82.2(2.4)}& \textbf{100.0(0.0)}\\
EXP3(0.2)& 69.1(2.9)& 92.1(1.7)& 53.1(3.1)& 83.9(2.3)& 75.1(2.7)& 99.8(0.3)\\
RM(0.1)& 81.0(2.4)& 92.7(1.6)& 58.5(3.1)& 86.7(2.1)& 78.6(2.5)& 99.8(0.3)\\
RAND& 4.6(1.3)& 28.8(2.8)& 5.8(1.4)& 1.7(0.8)& 3.1(1.1)& 71.1(2.8)\\
\hline
\end{tabular}

\end{scriptsize}
\caption{Win-rate in head-to-head matches of Pursuit evasion game with time limit of 15 moves and $10\times 10$ grid board.}\label{fig:matches:peg}
\end{table}


Finally, we compared algorithms on the pursuit-evasion game on an empty $10\times 10$ grid with $15$ moves time limit and 10 different randomly selected initial positions of the units. The branching factor is up to 12, causing the number of leaf nodes to be less than $10^{16}$.

The results in Table~\ref{fig:matches:peg} show that the game is strongly biased towards the first player, which is the evader. The self-play results on the diagonal shows that $\doab$ won over 80.9\% matches against itself as the evader. Adding more computational time typically improved the play of the pursuer in self-play. This is caused by more complex optimal strategy of the pursuer. This optimal strategy is more difficult to find due to a larger branching factor (recall that pursuer controls two units) and requirement for more precise execution of strategies (a single move played incorrectly can cause an escape of the evader and can result in losing the game due to the time limit).

Let's first look at the differences in the performance of the algorithms on the side of the pursuer, which are more consistent. We need to compare the different columns, in which the pursuer ties the minimize the values.
The clear winner is UCT, which generally captured the evaders in approximately 40\% of the matches.
The second best pursuer is $\doab$ and the weakest is OOS, which captured the non-random opponent in less than 10\% cases.

The situation is less clear for the evader. Different algorithms performed best agains different opponents.
UCT was best against OOS, Exp3 and RM, but $\doab$ was the best against UCT and itself.
Exp3 is the weakest evader.

\subsubsection{Parameter tuning}\label{sec:eval:online:tuning}

\begin{table}
\centering
\begin{scriptsize}

Goofspiel: 13 cards, unknown point card sequence
\begin{tabular}{|lr|ccccc|c|}\hline
&&$\doab$&OOS(0.6)&UCT(2)&EXP3(0.2)&RM(0.1)&Mean\\\hline
OOS&0.5&49.4(3.2)&50.2(3.0)&54.4(4.2)&54.9(3.0)&49.4(3.0)&51.66\\
OOS&0.4&49.2(3.2)&50.5(3.0)&56.4(4.2)&54.1(3.0)&47.5(3.0)&51.54\\
OOS&0.3&48.2(3.2)&47.6(3.0)&58.4(4.2)&54.3(3.0)&48.0(3.1)&51.3\\
OOS&\textbf{0.2}&48.4(3.2)&50.2(3.0)&58.7(4.2)&54.3(3.0)&47.9(3.0)&\textbf{51.9}\\
OOS&0.1&51.0(3.2)&47.4(3.1)&53.4(4.3)&48.6(3.0)&43.9(3.0)&48.86\\
UCT&1.5&40.7(3.0)&45.4(3.0)&52.4(3.2)&53.9(3.9)&39.4(4.6)&46.36\\
UCT&1&39.6(3.0)&49.9(3.0)&58.3(3.2)&56.1(3.8)&43.1(4.6)&49.4\\
UCT&0.8&36.6(2.9)&51.1(3.0)&60.8(3.2)&59.7(3.8)&46.8(4.7)&51\\
UCT&\textbf{0.6}&39.9(3.0)&53.9(3.0)&61.2(3.1)&62.3(3.8)&46.6(4.7)&\textbf{52.78}\\
UCT&0.4&39.0(3.0)&54.9(3.0)&61.6(3.1)&58.6(3.8)&49.5(4.8)&52.72\\
EXP3&0.5&47.9(3.4)&42.6(3.0)&44.4(3.0)&47.4(3.0)&40.1(3.0)&44.48\\
EXP3&0.4&48.4(3.4)&44.8(3.0)&48.4(3.0)&49.5(3.0)&39.5(3.0)&46.12\\
EXP3&\textbf{0.3}&46.0(3.4)&44.5(3.0)&51.8(3.0)&51.1(3.0)&41.0(3.0)&\textbf{46.88}\\
EXP3&0.2&40.5(3.4)&47.2(3.0)&47.6(3.0)&50.0(3.0)&41.3(3.0)&45.32\\
EXP3&0.1&42.3(3.4)&44.9(3.0)&48.9(3.0)&51.2(3.0)&40.9(3.0)&45.64\\
RM&0.5&42.2(4.2)&44.9(3.0)&43.9(3.0)&46.9(3.0)&42.4(3.0)&44.06\\
RM&0.3&42.6(4.3)&49.3(3.0)&57.9(2.9)&53.9(3.0)&48.5(3.0)&50.44\\
RM&0.2&38.9(3.8)&50.7(3.0)&63.8(2.9)&57.8(3.0)&48.2(3.0)&51.88\\
RM&\textbf{0.1}&41.3(4.2)&54.1(3.0)&61.2(2.9)&58.1(3.0)&51.2(3.0)&\textbf{53.18}\\
RM&0.05&43.0(4.3)&51.6(3.0)&60.1(2.9)&59.0(3.0)&49.0(3.1)&52.54\\
\hline
\end{tabular}

Oshi Zumo: 50 coins, 2$\cdot$3 + 1 positions, win rate, evaluation function
\begin{tabular}{|lr|ccccc|c|}\hline
&&$\doab$&OOS(0.6)&UCT(2)&EXP3(0.2)&RM(0.1)&Mean\\\hline
OOS&0.5&41.9(3.6)&50.9(3.6)&28.5(3.3)&54.9(3.6)&43.7(3.5)&43.98\\
OOS&0.4&40.1(3.6)&56.0(3.6)&26.6(3.2)&56.1(3.6)&42.6(3.6)&44.28\\
OOS&\textbf{0.3}&42.8(3.6)&57.8(3.5)&27.7(3.2)&55.7(3.6)&44.8(3.6)&\textbf{45.76}\\
OOS&0.2&39.5(3.6)&53.1(3.6)&26.8(3.2)&54.1(3.6)&41.4(3.5)&42.98\\
OOS&0.1&38.9(3.6)&55.6(3.6)&24.1(3.1)&56.2(3.6)&43.0(3.6)&43.56\\
UCT&1.5&88.9(2.5)&74.0(3.8)&79.1(2.9)&87.4(2.9)&70.6(3.9)&80\\
UCT&1&90.4(2.4)&74.8(3.7)&81.4(2.7)&89.8(2.6)&68.8(4.0)&81.04\\
UCT&\textbf{0.8}&94.3(1.8)&77.9(3.6)&77.1(3.0)&89.2(2.7)&74.1(3.8)&\textbf{82.52}\\
UCT&0.6&98.8(0.8)&75.7(3.7)&54.9(3.9)&90.0(2.6)&74.1(3.7)&78.7\\
UCT&0.4&97.7(1.2)&75.0(3.7)&31.4(3.7)&89.8(2.6)&70.6(3.9)&72.9\\
EXP3&\textbf{0.8}&59.2(3.9)&68.4(3.6)&23.0(3.1)&74.2(3.4)&61.5(3.7)&\textbf{57.26}\\
EXP3&0.6&57.8(3.9)&67.6(3.7)&20.4(3.1)&65.4(3.7)&59.4(3.8)&54.12\\
EXP3&0.5&50.9(4.0)&60.9(3.8)&15.1(2.7)&64.7(3.7)&52.9(3.9)&48.9\\
EXP3&0.4&47.4(4.0)&57.5(3.9)&17.5(3.0)&64.1(3.8)&54.9(3.9)&48.28\\
EXP3&0.3&41.2(3.9)&51.3(3.9)&14.3(2.7)&60.9(3.8)&49.8(3.9)&43.5\\
RM&0.5&46.4(3.7)&41.1(3.5)&31.7(3.3)&49.4(3.6)&34.3(3.3)&40.58\\
RM&0.3&49.4(3.7)&52.1(3.5)&33.8(3.4)&61.2(3.5)&43.7(3.5)&48.04\\
RM&0.2&46.4(3.7)&55.7(3.6)&30.7(3.3)&59.2(3.5)&46.4(3.6)&47.68\\
RM&\textbf{0.1}&42.3(3.6)&58.1(3.5)&34.9(3.4)&57.6(3.6)&54.1(3.6)&\textbf{49.4}\\
RM&0.05&43.1(3.6)&59.6(3.5)&29.7(3.3)&59.3(3.5)&51.1(3.6)&48.56\\
\hline
\end{tabular}

\end{scriptsize}
\caption{Parameter tuning for Goofspiel with 13 cards and unknown point cards sequence.}\label{fig:tuning}
\end{table}

In general, the exploration parameter can have significant influence on the performance of the algorithm.
We choose the parameters individually for each domain by running mutual matches with a preselected fixed pool of opponents.
In includes $\doab$ and each of the sampling algorithms with one setting of the parameter selected based on the results of the offline experiments.
These values are 0.6 for OOS, 2 for UCT, 0.2 for Exp3 and 0.1 for RM.
For each domain, we created a table such as the one for Goofspiel presented in Table~\ref{fig:tuning}.
We then picked the parameter for the final cross table as the parameter with the best mean performance against all the fixed opponents.

In the presented variant of Goofspiel, the choice of the exploration parameter has a very small influence on the performance against $\doab$.
The differences are more noticeable in matches against other algorithms, but since the optimal parameters are often different for different opponents, the mean performance presented in the last column does not vary a lot for different setting.

The differences between various parameter settings are larger in smaller games and mainly if the evaluation function is used.
We present the results for Oshi Zumo.
For OOS, the exploration parameter of $0.3$ is consistently the best against all opponents, with the exception of Exp3, which looses slightly more to OOS with exploration 0.4.
However, the difference is far from significant even after 1000 matches.
The differences in performance of UCT with different parameters are more often statistically significant.
Overall, the best parameter is 0.8, even though the performance is significantly better against $\doab$ with smaller exploration and against UCT(2) with higher exploration.
The best performance for Exp3 was surprisingly achieved with very high exploration. The best of the tested values was 0.8, which means that 80\% of tie, the next sample is selected randomly regardless of the collected statistics about move qualities.
The higher values were consistently better for all opponents. Even higher values might perform slightly better, but it would almost certainly not reach the performance of UCT and even this setting performs significantly better than the best settings for the remaining two algorithms.
RM seems to be quite sensitive to the parameter choices in this domain and the results for specific opponents are more inconclusive that for other algorithms.
When playing $\doab$, RM wins 7\% more  matches with parameter $0.3$ than with the overall optimal $0.1$.
On the other hand, when playing OOS, even smaller parameter value would be preferable.

The presented parameter tuning table are very representative of the behaviour of the algorithms with different parameters. The choices of the optimal parameters generally depend much more on the domain than the selected opponent, but in some cases the optimal choice for one opponent is far from optimum with the other opponent.  Especially with Exp3 and UCT, very different parameters are also optimal for different domains.
While in the presented results in Oshi Zumo with evaluation function, even 0.8 seems to be too low for Exp3, in Tron with evaluation function, the optimal parameter for Exp3 is 0.1.
The range of optimal parameters is much smaller for OOS and and RM, which were always between 0.1 and 0.3. This can be a notable advantage for playing previously unknown games without sufficient time to tune the parameters for the specific domain.

\subsubsection{Summary}

\begin{table}
\centering
\scriptsize
%\renewcommand{\arraystretch}{0.5}%
\begin{tabular}{|r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|r@{\hspace{7pt}}r|}\hline
&\multicolumn{2}{|c|}{Oshi}&\multicolumn{2}{|c|}{OshiE}&\multicolumn{2}{|c|}{GS}&\multicolumn{2}{|c|}{Rand}&\multicolumn{2}{|c|}{PE}&\multicolumn{2}{|c|}{Tron}&\multicolumn{2}{|c|}{TronE}\\
&$\bar{d}$&Its.&$\bar{d}$&Its.&$\bar{d}$&Its.&$\bar{d}$&Its.&$\bar{d}$&Its.&$\bar{d}$&Its.&$\bar{d}$&Its.\\\hline
OOS&5.5&187k&5.8&349k&11.2&73k&9.7&93k&8.1&28k&17.7&77k&17.4&265k\\
RM&7.1&186k&7.6&244k&17.8&92k&12.3&108k&11.0&34k&26.3&75k&25.1&251k\\
UCT&4.0&134k&6.5&213k&9.6&74k&8.0&94k&7.0&30k&15.1&72k&16.7&279k\\
Exp3&3.3&14k&3.7&12k&12.0&50k&8.4&95k&7.6&32k&17.5&80k&18.3&275k\\
\hline
\end{tabular}
\caption{The number of iterations (in thousands) and  the mean depth($\bar{d}$) of the tree constructed by the sampling algorithms in 5 seconds in the root node of the evaluated games.}\label{fig:tab:depthsIters}
\end{table}


Several conclusions can be made form the head-to-head comparison of the algorithms in larger games.
First, the fast convergence and low exploitability of OOS in the smaller variants of the games is not a very good predictor of its performance in the online setting. OOS was consistently one of the worst performing algorithms. One possible reason could be that with higher exploration parameter required for faster convergence, the part of the game tree constructed by OOS is too wide and shallow, and does not allow it to find important situations later in the game. However, Table~\ref{fig:tab:depthsIters} shows that while OOS builds a substantially less deep tree as RM, it is often deeper than for other MCTS algorithms which performed better. If we set the exploration parameter to smaller numbers, and also generally deeper in the tree, the importance sampling correction of regret updates causes very high variance of their values, which generally slows-down the convergence to good solutions and may result in building the tree in less useful directions.

Second, while $\doab$ with a good evaluation function wins over the sampling algorithms without domain specific knowledge, it is not necessary the case with a weaker evaluation function, as we can see in Goofspiel. Moreover, when the sampling algorithms are allowed to use the evaluation functions, $\doab$ is typically outperformed by either RM in Tron or UCT in Oshi-Zumo.
Evaluation function improves the performance of OOS the least of all sampling algorithms. \bbosansky{why?}

Third, when the algorithms have more time for finding the move to play, the differences between win-rates of the sampling algorithms are getting smaller. Longer thinking time has also the same effect in the parameter tuning and it also significantly improves the performance of the sampling algorithms against backward induction. This is expected, since the difference is too small for the $\doab$ algorithm to reach greater depth, while it is sufficient for the sampling algorithms to execute $5$ times more iterations that improves their strategy.

Fourth, Exp3 in its standard form is usable only in games with smaller branching factors. The cost of computing the exponential for each action causes an order of magnitude smaller number of iterations in Oshi-Zumo, compared to all other sampling algorithms.

Finally, RM seems to be consistently the best performing algorithm in game playing for this class of games. Even with similar number of iterations, it builds a substantially deeper tree (see Table~\ref{fig:tab:depthsIters}), which indicates the algorithm is able to better target the search to important parts of the tree. Out of all the games and both time settings, it loses only in Oshi-Zumo with evaluation function, where it is outperformed by UCT.

%Discussion:
%-low exploitability not necessary correlated to quality of play, possibly other solution concept, such as non-dominated strategies.
%-sampling algorihtms perform better compared to $\doab$ and coloser to each other with more time per move
%- evaluatin functions help, but they help OOS less
%- OOS did not proove to be good even though it converges the fastest
%- RM is very consistently the overall winner with UCT to consider with evaluation function
%- more time causes less sensitivity to parameters
