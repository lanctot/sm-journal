
In this paper, we provide an extensive survey of existing literature on solving and playing zero-sum extensive-form games with perfect information and simultaneous moves. Afterwards, we explain in detail several algorithms for computing optimal strategies and game playing in these games. Along with the novel algorithms we have introduced in past years, namely $\doab$, $\biab$, OOS and MCTS with regret matching selection, we explain also the most frequently used pre-existing algorithms for this class of games.

We empirically compare the performance of these algorithms on five substantially different games in two settings. In offline equilibrium computation setting, we show that the novel backward-induction-based $\doab$ algorithm is able to prune-out wast majority of search space. In most games, it is orders of magnitude faster than classical backward induction and it is never significantly out-performed by any of its competitors in computing exact equilibria. In the offline setting, the only benefit of the sampling algorithms can be in rough approximation of the equilibrium solution in very short time. Their results are often inconsistent with such short computation times, but after a longer computation, it is clear that OOS converges to the Nash equilibrium the fastest.

The speed of convergence to the equilibrium form the offline setting is surprisingly not a very good indicator of the game playing performance in online setting of head-to-head matches. OOS did not prove to be the best there. The $\doab$ algorithm cannot work without a domain-specific evaluation function in this setting and its performance heavily depends on its quality. With a good evaluation function, it outperforms the sampling algorithms if they do not use any domain-specific knowledge. However, if even the sampling algorithms are allowed to use the evaluation functions, they always significantly outperform $\doab$. Therefore, we conclude that sampling algorithms are a better choice for game playing in this class of games. Among them, MCTS based on regret matching performed consistently the best in our experiments. It was outperformed only by MCTS with UCT selection in the game of Oshi-Zumo with an evaluation function. For UCT, we have shown that adding randomization to this algorithm can be crucial in this class of games.

This work opens several interesting directions for future research. After introducing a strong pruning algorithm, it is of interest to formally study the limitations of pruning  for this class of games, similarly to the theory developed for games with sequential moves. Future work could show if the introduced pruning techniques can be substantially improved or if they are in some sense optimal.
Furthermore, running large head-to-head tournaments for evaluating game playing performance is very time consuming and it often does not give a lot of insight into the reasons behind the results. Proximity to the Nash equilibrium does not seem to be a good indicator of game playing performance; hence, it is interesting to study alternative measures of quality of the algorithms that would better predict their actual performance in large games. \\

\noindent {\bf Acknowledgements.} This work is partially funded by the Czech Science Foundation (grant no. P202/12/2054), the Grant
Agency of the Czech Technical University in Prague (grant no. OHK3-060/12), and the Netherlands
Organisation for Scientific Research (NWO) in the framework of the project Go4Nature, grant number 612.000.938. 
The access to computing and storage facilities owned by parties and projects contributing to the National Grid 
Infrastructure MetaCentrum, provided under the 
programme ``Projects of Large Infrastructure for Research, Development, and Innovations'' (LM2010005) is highly appreciated.

