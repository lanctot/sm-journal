There has been a number of algorithms designed for simultaneous move games that can be classified into three categories: 
(1) iterative learning algorithms, 
(2) exact backard induction algorithms, 
(3) approximative sampling algorithms.
The first type computes strategies through iterated self-play.
The second type computes a Nash equilibrium strategy of the game. 
The third type computes strategies by approximating utilities using sampling. 

\subsection{Iterative Learning Algorithms}

% mlanctot: I put this first because I thought historically it kind of made more sense.
%           Also, then it kind of leads into the ones that we will look at. 

% http://students.cs.byu.edu/~cs670ta/Fall2009/MinimaxQLearning.pdf
% http://www.ualberta.ca/~szepesva/papers/ml96.ps.pdf
% http://www.cogsci.rpi.edu/~rsun/si-mal/article3.pdf
% http://www.cs.duke.edu/~parr/uai2002.ps.gz

A significant amount of interest in simultaneous move games was generated by initial work 
on multiagent reinforcement learning. In multiagent reinforcement learning, each agent acts simultaneously and 
the joint action determines how the state changes. Littman introduced Markov games to model these interactions 
as well as a variant of Q-learning called Minimax-Q to compute strategies~\cite{Littman94markovgames,Littman01Value}.
Minimax-Q modifies the learning rule so that the value of the next state (the subgame) is obtained by solving
a linear program using the estimated values of that subgame's root.
As is common in these settings, the goal of each agent is to maximize their expected utility. 
In two-player zero-sum Markov games, an optimal policy corresponds to a Nash equilibrium strategy, which assures the agent 
the highest worst-case expected payoff. Initial results provided conditions under which approximate dynamic 
programming could be used to guarantee convergence to the optimal value function and 
policies~\cite{Littman96ageneralized}. Later, in \cite{Lagoudakis02}, Lagoudakis \& Parr provided stronger bounds 
and convergence guarantees for least squares temporal different learning using linear function approximation. 

At around this same time period, gradient ascent methods were introduced for playing 
repeated games~\cite{Singh20Nash,Bowling01WoLF}. These algorithms update strategies in a direction of the strategy space 
that increases expected payoff with respect to the opponent's strategy. These were then generalized and combined, and 
shown to minimize regret over time~\cite{Zinkevich03Online,Bowling05Convergence}, leading to strong convergence 
guarantees in multiagent learning. More no-regret algorithms followed and were applied to an imperfect information 
games in sequence-form (One-Card Poker)~\cite{Gordon06No}. Soon later, counterfactual regret (CFR) minimization was 
introduced for large imperfect information games~\cite{CFR}. CFR has gained much attention due to its success in 
computing Poker AI strategies, and in this paper we analyze the effectiveness of a specific form of Monte Carlo CFR 
for the first time in simultaneous move games. 

As we focus on zero-sum simultaneous move games in this paper, the work on multiagent learning in general-sum and 
cooperative games has been omitted. A modern survey of the relevant previous work in multiagent 
reinforcement learning and game theory (including the zero-sum case) is presented in~\cite{Nowe12MARLchapter}. 

\subsection{Exact Backward Induction Algorithms}

The techniques in this section are based on the backward induction algorithm (cf. \cite{Shoham09}), 
a form of dynamic programming~\cite{Bellman57} often presented for purely sequential games. 
A slightly modified variant of the algorithm can also be applied to simultaneous move 
games (e.g., see \cite{Ross71Goofspiel,buro2003,Rhoads12Computer}). 
%\bbosansky{We do need some better reference here.}) 
% mlanctot: added the Ross 1971 paper; as far as I know it's the earliest one. Would be nice to have a text book cover this.
The algorithm searches through the game tree in the depth-first manner and after computing the values of all the succeeding subgames, it solves the normal-form game corresponding to this state (i.e., computes a NE of the matrix game in the current state of the game), and propagates the calculated game value to the predecessor. The result of the backward-induction algorithm is a refinement of NE called \emph{subgame-perfect Nash equilibrium}. 

There are two notable algorithms that improve the standard backward induction in simultaneous move games. 
First is an algorithm by Saffidine et al.~\cite{Saffidine12SMAB} termed simultaneous move alpha-beta algorithm (SMAB). 
The main idea of the algorithm is to reduce the number of the recursive calls of the backward-induction algorithm by removing dominated actions in every stage game. The algorithm keeps bounds on the utility value for each successor in a game state. 
The lower and upper bounds represent the threshold values, for which neither of the actions of the player is dominated by any other action in the current matrix game. These bounds are calculated by linear programs in the state given existing exact values (or appropriate bounds) of the utility values of all the other successors of the state. If they form an empty interval (the lower bound is higher than the upper bound), pruning takes place and the dominated action is no longer considered in this state afterward. 
SMAB outperforms classical backward induction, however, the computational speed-up is only marginal. 

% mlanctot: This seems overly negative toward SMAB and I don't think we need to justify ourselves at this point in the paper. 
%           How about this.. in the evaluation section (next to the more impressive DOAB result) let's put something like:
%           Note: this saving is several orders of magnitude more than reported by SMAB~\cite{Saffidine12SMAB}. 
%SMAB outperforms classical backward induction, however, the computational speed-up is limited. 
%The reason is that computing dominated actions is a costly operation that does not prune many actions; hence, much of the game tree is still fully evaluated. The authors propose further enhancements in their work~\cite{Saffidine12SMAB}, however, these improvements are domain-specific heuristics and do not significantly change the overall performance of the algorithm. Since other exact algorithms achieve computation speed-up in several orders of magnitude compared to backward induction (as we show in Section~\ref{sec:eval}), we do not explicitly use SMAB in our experiments.
% bbosansky: Ok, I agree.

The second exact algorithm that is significantly faster compared to the classical backward induction was introduced in~\cite{Bosansky13Using}.
This algorithm is described in detail in Subsection~\ref{sec:doab}. The main idea is to integrate two key components: (1) instead of evaluating all successors in each state of the game and solving a normal-form game, the algorithm exploits the iterative framework known in game theory as double-oracle algorithm~\cite{McMahan03Planning}; (2) the algorithm computes bounds on the utility values of the successors by serializing the subgames and running classical alpha-beta algorithm. 

Finally, since simultaneous move games can be seen as standard extensive-form games with imperfect information, one can use techniques 
designed for large imperfect information games. 
An algorithm that is also built on double-oracle is the Range-of-Skill algorithm~\cite{Zinkevich07New}.  
However, the number of iterations required by this algorithm in the worst case can be large~\cite{Hansen08On}. 
There are also state-of-the-art algorithms for solving generic extensive-form games with imperfect information, based on sequence-form 
optimization problems~\cite{koller1996,Sandholm10The,bosansky2013-aamas}. 
However, these algorithms do not exploit the specific structure of simultaneous move games and could require memory that is linear 
in the size of the game tree. In practice, this prohibits scaling to larger games (see, \eg \cite{Saffidine12SMAB}) and causes weak performance
compared to tailored algorithms.

%\mlanctot{We should mention the Range-of-Skill algorithm \cite{Zinkevich07New} somewhere in this subsection. It was shown that there are very bad worst cases even for simple games \cite{Hansen08On}.. do we know if such bad cases are less likely to occur in simultaneous move games? It'd be nice if we could say anything at all on this.}\bbosansky{Not exactly sure how to approach this ... I have never really understood the benefits of ROS well.}

\subsection{Approximative Sampling Algorithms} \label{sec:related:sampling}

Monte Carlo Tree Search (MCTS) is a simulation-based state space search technique often used in extensive-form games \cite{Coulom06,UCT}. 
Having first seen practical success in computer Go \cite{Gelly2011,Gelly12}, MCTS has since been applied successfully to simultaneous move games and imperfect information games as well~\cite{Ciancarini10Kriegspiel}. 
Most of the successful applications use classical exploration/exploitation formula Upper Confidence Bounds (UCB)~\cite{UCB} as a selection strategy. These variants of MCTS are also known as UCT (UCB applied to trees). The first application of MCTS to simultaneous move games was in general game playing (GGP) \cite{GGP} programs: {\sc Cadiaplayer} \cite{Cadiaplayer,Finnsson12} uses UCB selection strategy for each player in a single game tree. The success of MCTS algorithm was demonstrated by success of {\sc Cadiaplayer} which was the top-ranked player of the GGP competition between 2007 and 2009, and also in 2012.

Despite this success, Shafiei et al. in \cite{Shafiei09} provide a counter-example showing that this straightforward application of UCT does not
converge to NE even in the simplest simultaneous move games and that a player playing a NE can exploit this strategy. Another variant of UCT, which has been applied to  Tron~\cite{Samothrakis10Tron}, builds the tree as if the players were moving sequentially giving one of the players an informational advantage. This approach also cannot converge to NE in general. For this reason, other variants of MCTS were considered for simultaneous move games. Teytaud and Flory describe a search algorithm for games with short-term imperfect information~\cite{Teytaud11Upper}, which are a generalization of simultaneous move games. Their algorithm uses a different selection strategy, called Exp3~\cite{Auer2003Exp3},
and was shown to work well in the Internet card game Urban Rivals. We provide details of these two main existing selection functions in Subsections~\ref{sec:duct} and~\ref{sec:exp3}.
A more thorough experimental investigation of different selection policies including UCB, UCB1-Tuned, UCB1-greedy, Exp3, and more is reported in the game of Tron \cite{Perick12Comparison}. The work by Lanctot et al.~\cite{Lanctot13Goofspiel} compares some of these variants and proposes Online Outcome Sampling, a search version of Monte Carlo CFR~\cite{Lanctot09Sampling}, which computes an approximate equilibrium strategy with high probability. We describe this algorithm in Subsection~\ref{sec:oos}.
Finally, Lisy et al.~\cite{lisy2013-nips} present variants of MCTS that provably converge to Nash equilibria in simultaneous move games, in general, using any regret-minimizing algorithm at each stage, showing observed worst-case behavior in several cases. 

There have been two recent studies that examine the head-to-head performance of these variants in practice. 
The first builds on previous work in Tron~\cite{Lanctot13Tron} by varying the shape of the initial board, 
comparing previous serialized variants of simultaneous move MCTS. The authors found that UCB1-Tuned worked 
particularly well in Tron when using knowledge-based playout policies. The success of UCB1-Tuned differed in 
a similar study of the same variants across nine domains~\cite{Tak14smmcts} without domain knowledge. In this 
work, the chosen games were ones inspired by previous work in general game playing and did not include chance elements. 
Results indicate that parameter-tuning landscapes do not seem as smooth as in the purely sequential case. 

% mlanctot: said enough about this paper already...
%Generally, head-to-head performance of UCB variants perform well, despite their theoretical shortcomings, with Oshi Zumo and 
%Goofspiel being notable exceptions where MCTS using a regret matching selection policy performs particularly well.

\subsubsection{Simulation-based Search in Real-time Games}

%\mlanctot{Brief overview of relevant work in RTS/video games}

Real-time games are not turn-based and represent a realistic physical situations where agents can move freely in space. 
The state of the game is a continuous function of time and the effect of some actions may only be realized some time 
after the decision is made. These games are often appropriately modeled as a simultaneous move game with very short 
delays (40 milliseconds) between frames. 

MCTS has enjoyed some success in these types of games, in the single-agent 
setting~\cite{Pepels14Monte,Perez14PTSP} and multiagent setting~\cite{Balla09UCT}, much of this work inspired by video 
games~\cite{Cowling13Video,BellemareNVB13,Ontanon13RTSSurvey}. Few of these works have considered MCTS
in the simultaneous move game directly. 
In one of the first papers on real-time strategy games, the authors used randomized serialization 
of the game~\cite{kovarsky2005heuristic}, or strategy simulation
from scripts was used to build a single matrix of values from which an equilibrium strategy was 
computed using linear programming~\cite{Sailor07adversarial}.  
This search can be extended to multiple nodes where internal nodes would correspond to scripts being interrupted to replan, similarly to \cite{lisy2009gbgts}.
MCTS-style multistage replanning was also applied to a real-time battle scenario which was also accurately
represented as a discrete simultaneous move game~\cite{Beard12Using}. Results of this work show and that multistage
forward replanning can improve upon single-stage forward planning, and can produce approximate Nash equilibrium strategies
when mixed strategies are computed at each stage during search.
Around the same time, a serialized (sequential) version of alpha-beta was proposed for simultaneous move games 
and run on combat scenarios~\cite{Churchill2012Fast}; this algorithm is described in greater detail in
Section~\ref{sec:algs:biab} as it forms the basis of the follow-up enhanced by
double-oracle, presented in Section~\ref{sec:algs:doab}.

In this paper, we focus on the analysis of different algorithms for two-player simultaneous move games. Therefore, 
problems arising from discrete modeling of continuous time and space remain outside the scope of this paper.

%The benefit of serializing the game is that bounds
%on the correct minimax value can be obtained for the underlying simultaneous move game. These bounds can be used to cut out
%parts of the game tree in backward induction algorithms, and is explained in detail in Section~\ref{sec:algs:biab}.





