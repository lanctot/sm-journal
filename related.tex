
\section{Related Work}

There is a number of algorithms designed for simultaneous-move games that can be classified into three categories: 
(1) iterative learning algorithms, 
(2) exact backard induction algorithms, 
(3) approximative sampling algorithms.
The first type computes strategies through iterated self-play.
The second type computes a Nash equilibrium strategy of the game. 
The third type computes strategies by approximating utilities using sampling. 

%\bbosansky{Add RW for Markov Games.}
% mlanctot: started on this

\subsection{Iterative Learning Algorithms}

% mlanctot: I put this first because I thought historically it kind of made more sense.
%           Also, then it kind of leads into the ones that we will look at. 

% http://students.cs.byu.edu/~cs670ta/Fall2009/MinimaxQLearning.pdf
% http://www.ualberta.ca/~szepesva/papers/ml96.ps.pdf
% http://www.cogsci.rpi.edu/~rsun/si-mal/article3.pdf
% http://www.cs.duke.edu/~parr/uai2002.ps.gz

There was a significant amount of interest in simultaneous moves games generated through initial work 
on multiagent reinforcement learning. In multiagent reinforcement learning, each agent acts simultaneously and 
the joint action determines how the state changes. Littman introduced Markov games to model these interactions 
as well as variant of Q-learning called MiniMaxQ to compute strategies~\cite{Littman94markovgames,Littman01Value}.
As is common in these settings, the goal of each agent is to maximize their expected utility. 
In zero-sum Markov games, an optimal policy corresponds to a Nash equilibrium strategy, which assures the agent 
the highest worst-case expected payoff. Initial results provided conditions under which approximate dynamic 
programming could be used to guarantee convergence to the optimal value function and 
policies~\cite{Littman96ageneralized}. Lagoudakis \& Parr later provided stronger bounds and convergence guarantees
for least squares temporal different learning using linear function approximation~\cite{Lagoudakis02}. 

At around this same time period, gradient ascent methods were introduced as for playing 
repeated games~\cite{Singh20Nash,Bowling01WoLF}. These algorithms update strategies in a direction of the strategy space 
that increases expected payoff with respect to the opponent's strategy. These were then generalized and combined, and 
shown to minimize regret over time~\cite{Zinkevich03Online,Bowling05Convergence}, leading to strong convergence 
guarantees in multiagent learning. More no-regret algorithms followed and were applied to an imperfect information 
games in sequence-form (One-Card Poker)~\cite{Gordon06No}. Soon later, counterfactual regret (CFR) minimization was 
introduced for large imperfect information games~\cite{CFR}. CFR has gained much attention due to its success in 
computing Poker AI strategies, and in this paper we analyze its effectiveness for the first time in simultaneous 
move games. 

As we focus on zero-sum simultaneous move games in this paper, the work on multiagent learning in general-sum and 
cooperative games has been omitted. A modern elaborate survey of the relevant previous work in multiagent 
reinforcement learning and game theory (including the zero-sum case) is presented in~\cite{Nowe12MARLchapter}. 

\subsection{Exact Backward Induction Algorithms}

The techniques in this section are based on the backward induction algorithm (cf. \cite{Shoham09}), 
a form of dynamic programming~\cite{Bellman57} often presented for purely sequential games. 
A slightly modified variant of the algorithm can also be applied to simultaneous move 
games (e.g., see \cite{Ross71Goofspiel,buro2003,Rhoads12Computer}). 
%\bbosansky{We do need some better reference here.}) 
% mlanctot: added the Ross 1971 paper; as far as I know it's the earliest one. Would be nice to have a text book cover this.
The algorithm searches through the game tree in the depth-first manner and after computing the values of all the succeeding sub-games, it solves the normal-form game corresponding to this state (i.e., computes a NE of the matrix game in the current state of the game), and propagates the calculated value to the predecessor. The result of the backward-induction algorithm is a refinement of NE called \emph{subgame-perfect Nash equilibrium}. 

There are two notable algorithms that improves the standard backward induction in simultaneous move games. 
Saffidine et al.~\cite{Saffidine12SMAB} termed simultaneous-move alpha-beta algorithm (SMAB). 
The main idea of the algorithm is to reduce the number of the recursive calls of the backward-induction algorithm by removing dominated actions in every stage game. The algorithm keeps bounds on the utility value for each successor in a game state. 
The lower and upper bounds represent the threshold values, for which neither of the actions of the player is dominated by any other action in the current matrix game. These bounds are calculated by linear programs in the state given existing exact values (or appropriate bounds) of the utility values of all the other successors of the state. If they form an empty interval (the lower bound is higher than the upper bound), a pruning occurs and the dominated action is not considered in this state any more. 

SMAB outperforms classical backward induction, however, the computation speed-up is not that dramatic. 
The reason is that computing dominated actions is a costly operations that does not prune many actions; hence, a lot of the game tree is still fully evaluated. The authors propose further enhancements in their work~\cite{Saffidine12SMAB}, however, these improvements are heuristic and do not significantly change the overall performance of the algorithm. Since other exact algorithms achieve computation speed-up in several orders of magnitude compared to backward induction (as we will show in experimental section of this chapter), we do not explicitly use SMAB in our experiments.

The second exact algorithm that is significantly faster compared to the classical backward induction was introduced in~\cite{Bosansky13Using}.
This algorithm is described in details in Section~\ref{sec:doab}, however, the main idea is to integrate two key components: (1) instead of evaluating all successors in each state of the game and solving a normal-form game, the algorithm exploits the iterative framework known in game theory as double-oracle algorithm~\cite{McMahan03Planning}; (2) the algorithm computes bounds on the utility values of the successors by serializing the sub-games and running a standard alpha-beta algorithm. 

Finally, since simultaneous-move games can be seen as standard extensive-form games with imperfect information, one can use state-of-the-art algorithms for solving generic extensive-form games with imperfect information that are based on sequence form linear programming~\cite{koller1996,bosansky2013-aamas}. However, these algorithms do not exploit the specific structure of simultaneous-move games causing, as we will show in the experiments, weak performance in practice.

\subsection{Approximative Sampling Algorithms}

Monte Carlo Tree Search (MCTS) is a simulation-based state space search technique often used in extensive-form games \cite{Coulom06,UCT}. 
Having first seen practical success in computer Go \cite{Gelly12}, MCTS has since been applied successfully to simultaneous-move games as well. 
Most of the successful applications use classical exploration/exploitation formula Upper Confidence Bounds (UCB)~\cite{UCB} as a selection strategy. These variants of MCTS are also known as UCT (UCB applied on trees). The first application of MCTS to simultaneous move games was in general game playing (GGP) \cite{Cadiaplayer} programs: the Cadiaplayer \cite{Cadiaplayer,Finnsson12} uses UCB selection strategy for each player in a single game tree. The success of MCTS algorithm was demonstrated by success of Cadiaplayer that was the top performing player of the GGP competition
between 2007 and 2009. 

Despite this success, Shafiei et al. in \cite{Shafiei09} provide a counter-example showing that this straightforward application of UCT does not
converge to NE even in the simplest simultaneous move games and that a player playing a NE can exploit this strategy. Another variant of UCT, which has been applied to the simultaneous move game Tron~\cite{Samothrakis10Tron}, builds the tree as if the players were moving sequentially giving one of the player unrealistic informational advantage. This approach also cannot converge to NE in general. For this reason, other variants of MCTS were considered for simultaneous move games. Teytaud and Flory describe a search algorithm for games with short-term imperfect information~\cite{Teytaud11Upper}, which are a generalization of simultaneous move games. Their algorithm uses a different selection strategy called Exp3~\cite{Auer2003Exp3} for the simultaneous moves and was shown to work well in the Internet card game Urban Rivals. 
A more thorough experimental investigation of different selection policies including UCB, UCB1-Tuned, UCB1-greedy, Exp3, and more is reported in the game of Tron \cite{Perick12Comparison}. The work of by Lanctot et al.~\cite{Lanctot13Goofspiel} compares some of these variants and proposes Online Outcome Sampling, a search version of Monte Carlo CFR~\cite{Lanctot09Sampling}, which computes an approximate equilibrium strategy with high probability. Lisy et al.~\cite{lisy2013-nips} present variants of MCTS that provably converge to Nash equilibria in simultaneous move games, in general, using any regret-minimizing algorithm at each stage, showing observed worst-case behavior of in several cases. 

%\mlanctot{TODO: Add summary and ref to new papers: BNAIC 2013 Tron paper, CIG 2014 GGP paper}

There have been two recent studies that examine the head-to-head performance of these variants in practice. 
The first builds on previous work in Tron~\cite{Lanctot13Tron} by varying the shape of the initial board, 
comparing previous serialized variants of simultaneous move MCTS. The authors found that UCB1-Tuned worked 
particularly well in Tron when using knowledge-based playout policies. The success of UCB1-Tuned differed in 
a similar study of the same variants across nine domains~\cite{Tak14smmcts} without domain knowledge. In this 
work, the chosen games were ones inspired by previous work in general game playing and did not include chance elements. 
Results indicate that parameter-tuning landscapes do not seem as smooth as in the purely sequence case. 
Generally, head-to-head performance of UCB variants perform well, despite their theoretical shortcomings, with Oshi Zumo and 
Goofspiel being notable exceptions where MCTS using a regret matching selection policy performs particularly well.

\subsubsection{Simulation-based Search in Real-time Games}

%\mlanctot{Brief overview of relevant work in RTS/video games}

Real-time games are not turn-based and represent a realistic physical situations where agents can move freely in space. 
The state of the game is a continuous function of time and the effect of some actions may only be realized some time 
after the decision is made. These games are often appropriately modeled as a simultaneous move game with very short 
delays (~40 ms.) between frames. 

MCTS has enjoyed some success in these types of games, in the single-agent 
setting~\cite{Pepels14Monte,Perez14PTSP} and multiagent setting~\cite{Balla09UCT}, much of this work inspired by video 
games~\cite{Cowling13Video,BellemareNVB13,Ontanon13RTSSurvey}. Few of these works have considered MCTS
in the simultaneous move game directly. 
However, in one of the first papers on real-time strategy games, strategy simulation
from scripts was used to build a single matrix of values from which an equilibrium strategy was 
computed using linear programming~\cite{Sailor07adversarial}.  
Using Simultaneous Move MCTS, presented in Subsection~\ref{sec:algs:smmcts}, this search could be extended 
to multiple nodes where internal nodes would correspond to scripts being interrupted to replan. 
In later work, alpha-beta search was run on a serialized (sequential) version of the 
simultaneous move game for combat scenarios~\cite{Churchill2012Fast}. The benefit of serializing the game is that bounds 
on the correct minimax value can be obtained for the underlying simultaneous move game. These bounds can be used to cut out
parts of the game tree in backward induction algorithms, as will be explained in detail in Subsection~\ref{sec:algs:biab}.





