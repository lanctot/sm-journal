In this section, we describe online adaptations of the algorithms described in the previous section and their application 
to any-time search within a limited time. 

\subsection{Iterative Deepening Backward Induction Algorithms} \label{sec:idbi}

Minimax search~\cite{AIbook} has been used with much success in sequential perfect information games, 
leading to superhuman strength in computer chess game play, one of the key advances of artificial 
intelligence~\cite{Campbell02deepblue}. 
Minimax search is an online application of backward induction run on approximated game. 
The game is approximated by searching to a fixed depth limit $d$, treating the states at depth $d$
as terminal states, evaluating their values using a heuristic evaluation function, $v(s)$. 
The main focus is to compute an optimal strategy for this heuristic approximation of the original game. 

Under limited time settings, a search algorithm is given a fixed time budget to compute a strategy. 
We use the classic approach of {\it iterative deepening}~\cite{AIbook} that runs several depth-limited 
minimax searches, starting at a low depth and iteratively increasing the depth of each successive search. 
Note that the depth limit of $d$ means that the algorithm evaluates $d$ joint actions (\ie pairs of simultaneous actions) possibly preceded by action of nature if present.  
%In sequential perfect information games, several enhancements can be applied due to researching the same 
%nodes, most of which are not immediately applicable in simultaneous move games. However, whether new 
%enhancements can be defined for this class of games remains an open research question. 
In our implementation of iterative deepening we follow a natural observation that a solution computed in state $s$ by player $i$ to depth $d$ contains depth $d-1$ solutions to all possible next states $\cT(s,a_i,?)$, where $a_i$ is the action selected for player $i$.
This allows us to start the iterative deepening in any state $s' \in \cT(s,a_i,?)$ with depth $d$.
If the time limit does not allow full evaluation of the sub-game with the root in $s'$, the algorithm reuses the result from previous computation.
In case there is no strategy stored for the state $s'$ due to pruning, iterative deepening is started with $d = 1$.

\subsection{Online Search using Sampling Algorithms}

\bbosansky{The following subsection is quite wordy -- feel free to make it more compact and technical.}
Using sampling algorithms in an online settings is simpler compared to the algorithms based on the backward induction, since no significant changes are needed and the algorithms do not need an evaluation function.
The algorithms are stopped after given time limit and the move to play or the complete strategy is extracted as described for each sampling algorithm in Section~\ref{sec:offline}.
There are two concepts that have to be discussed. 
First of all, the algorithms can re-use all information and statistics gained in previous iterations; hence, after returning a move and advancing to a succeeding state of the game $s'$, the subtree rooted in $s'$ of the incrementally built tree is preserved and used in the next iterations. 
Note that reusing the previously gathered statistics in the sub-tree rooted in $s'$ has no potentially negative effect on any variant of the MCTS algorithms since the behavior of the algorithm is exactly the same when the iteration is started in this node, or if this node is reached from its predecessor. On the other hand, this does not hold generally for OOS, where the regret values are weighted by the reach probabilities depending on the strategy used in the preceding nodes.  However, this fact does not negatively affect the convergence of the algorithm. In the definition of sampled counterfactual value in Equation~\ref{eq:scv}, the value of the leaf sampled in the current iteration is scaled up by one over the probability of reaching the leaf. This scaling also translates to regret values. After the root of the game is moved to a sub-tree and therefore the samples are shorter, it will generally have a larger sample probability and smaller weight in the regret updates. As a result, the earlier and less precise samples have larger weight in cumulative regrets than the later iterations. On the other hand, with larger sample probability, the regrets will be updated much more often, and eventually overweight the older samples.\vlisy{This is still a little unclear, but it might be the reason why OOS dost not work well online. I have implemented a modified version and I will run some experiments.}

Secondly, even though the sampling algorithms do not require to use domain-specific knowledge for online search, they often incorporate this type of knowledge to better guide the sampling and thus to evaluate more relevant parts of the state space~\cite{Gelly07Combining,Lorentz08Amazons,Winands09eval,Winands10MCTS-LOA,Winands11AB,Lorentz13Breakthrough,Lanctot14Implicit}. When directly comparing sampling-based algorithms with the backward-induction algorithms that are clearly dependent on the evaluation function, the outcome of such a comparison strictly depends on the quality of the evaluation function -- in case of a very large game, a backward induction algorithm with precise evaluation function will be significantly better (we will see such examples in the experimental evaluation). Therefore, we also use sampling algorithms combined with an evaluation function. The integration is done via replacing the random rollout  
by directly using the value of the evaluation function in the current state for MCTS or OOS algorithms; \ie Rollout($s$) in 
line~\ref{alg:smmcts:rollout} of Algorithm~\ref{alg:smmcts} or line~\ref{alg:oos:rollout} of Algorithm~\ref{alg:oos} is replaced by $eval(s)$. 
This has been commonly used in several previous works in Monte Carlo search~\cite{Lorentz08Amazons,Lorentz13Breakthrough,Lanctot14Implicit,RamanujanS11,Lanctot13MCMS}.

Again, such a modification does not generally affect theoretical properties of the algorithms -- the proofs of the convergence (e.g., \cite{lisy2013-nips}) assume that a whole game tree is eventually built and any statistics in the nodes collected before (either by random play-outs or evaluation functions) can eventually be over-weighted. For MCTS algorithms there is no reason to believe that a good evaluation function would give worse estimate of the quality of a sub-tree as a random play-out. The only complication can, again, be with the probabilities in OOS. The weight of the sample in equation~\ref{eq:scv} is lowered by the probability of reaching the sampled leaf from the updated information set. When using $\epsilon$-on-policy sampling, this ``tail'' probability is mostly cancelled by the probability of reaching the sampled leaf form the root in the denominator. If the sample is terminated earlier, it does not have a substantial effect on the scale of the regret updates.
