** First round of reviews, received early March 2015.
** Please do not edit this document. I have included reviews_1.txt that we can modify.

Ms. Ref. No.:  ARTINT-D-14-00249
Title: Algorithms for Computing Strategies in Two-Player Simultaneous Move Games
Artificial Intelligence

Dear Mr. Branislav Bosansky,

I attach reviews of the  above paper that you submitted to 'Artificial Intelligence'  for publication. As you can read, various issues have been raised by the reviewers and so your article must be revised before publication can be considered. Your revision will be sent out for a second review.
Please carefully address the issues raised in the comments.

If you are submitting a revised manuscript, please also:

a) outline each change made (point by point) as raised in the reviewer comments

  AND/OR

b) provide a suitable rebuttal to each reviewer comment not addressed

c) upload all source files of the revision.


To submit your revision, please do the following:

1. Go to: http://ees.elsevier.com/artint/

2. Enter your login details

3. Click [Author Login]
This takes you to the Author Main Menu.

4. Click [Submissions Needing Revision]

Please note that this journal offers a new, free service called AudioSlides: brief, webcast-style presentations that are shown next to published articles on ScienceDirect (see also http://www.elsevier.com/audioslides). If your paper is accepted for publication, you will automatically receive an invitation to create an AudioSlides presentation.

The revised version of your submission is due by Sep 04, 2015.

I look forward to receiving your revised manuscript.

Yours sincerely,

Rina Dechter
Editor-in-Chief
Artificial Intelligence- Now free access - See aij.ijcai.org

Reviewers' comments:

Daer Authors!

The two reviewers started being quite interested in the submission, but found too many technical issues that
prevent the paper from being progressed with a minor revision. I waited longer for a third one, but
by looking at the reviews and the paper several times the current decision became clear to me, so that
I truncated the process, since I no longer needed to rely on another opinion to come to a conclusion.

My own impression is that the paper tends to has a survey character than representing a
focused set of state-of-the-art original technical contributions from the authors. This of
course has pros and cons as survey submissions are perfectly fine for AIJ. But then, the paper
is not organized as a survey.

The first reviewer said that he doesn't miss theory, but my judgement is that you should increase
insights to the theory, e.g., on correctness issues or running time efficiencies. What about
probabilistic completeness?

As a result, I would like you to cristalize the contribution, elaborate on the theory/analysis,
follow the comments in detail and try a resubmission.



Reviewer #1: **********
* Part A *
**********

CONTENT
=======

Is the paper technically sound?
-------------------------------

YES. Mathematics and algorithmic descriptions are correct.


Is the work described original?
-------------------------------

YES. The paper is basically a survey of work by the same authors (plus some further collaborators who are not coauthors of the present paper) previously published at various conferences and workshops. On the one hand, the present paper contains no new algorithms or theoretical analyses beyond what has been previously published by its authors, but it does contain an extensive empirical comparison of their algorithms and some pre-existing algorithms (in particular, UCT), that is, to my knowledge, novel.


Does it show sufficient applicability to AI?
--------------------------------------------

YES. Definitely relevant.


Does it place new results in appropriate context with earlier work?
-------------------------------------------------------------------

YES. The algorithms discussed and evaluated also contain baseline backward induction (for exact solutions) and UCT and Exp3 (for approximate solutions). Related algorithmic literature is sufficiently discussed (maybe a bit too technically at times), although some discussion of existing theoretical results, especially regarding convergence of online approaches, would be appreciated.


Is the paper sufficiently novel? (A paper is novel if the results it describes
were not previously published by other authors, and were not previously published
by the same authors in any archival journal.)
---------------------------------------------------------------------------------

YES. See above (originality).


Is the paper complete? (Generally, a paper is complete if it includes all relevant proofs
and/or experimental data, a thorough  discussion of connections with the existing literature,
and a convincing discussion of the motivations and implications of the presented work.)
---------------------------------------------------------------------------------------------

YES (mostly). The paper is purely algorithmic and experimental, so the lack of theory and proofs is unproblematic (there is nothing to prove). What is do miss, though, is a more convincing discussion of what we really learn from the experimental results. We see numbers that tell us in which setting and which domain which algorithm performs best, but it is not always clear why this is the case. There are successful attempts in the paper at explaining some of the results informally, but I would have liked some more formal depth here. Reading the "future work" part of the paper, it becomes clear that the authors are aware of this limitation of their work. Still, it would be nice to know a bit more about the limitations of pruning using the various presented algorithms, and it would be nice to understand better why the fast convergence and low exploitability of OOS in the small game instances is such a poor predictor of its performance in the online setting. In Section 6.5.7,
the authors attempt at coming up with an explanation, but it is too speculative to be convincing.


Does anything need to be added or deleted?
------------------------------------------

NO. Well, except for a better/deeper explanation of the different OOS behavior in offline and online settings. But that might already be beyond the scope of the present article.


Is the result important enough to warrant publication?
------------------------------------------------------

YES. Both in the offline and in the online setting, (some of) the presented algorithms clearly advance the state of the art.


Accessibility (Is the paper accessible to the broad readers of AIJ? Specifically:
Is the problem addressed in the paper, described in an accessible manner to readers of the journal?
Is it possible for an AI researcher, outside the specific area,  to assess its relevance to  AI in
general and  to their own research area, in particular?
Is enough background given (e.g., prior work, preliminaries, methodology) so an AI researcher outside
the specific area can appreciate the paper's contribution?)
-----------------------------------------------------------------------------------------------------

YES. The structure of the paper is excellent and preliminaries are introduced well. Also, the different settings that are discussed (backward vs. forward search, online vs. offline search) become sufficiently clear. The descriptions of the algorithms themselves are also mostly very good and readable. The chapter on experiments starts out with a discussion of all the benchmark domains, and for each individual experiment, the methodology is clearly described.


FORM
====

Does the abstract adequately reflect the contents?
--------------------------------------------------

YES.


Does it contain adequate references to previous work?
-----------------------------------------------------

YES.


Does it explain clearly what was done that is new?
--------------------------------------------------

YES. When new concepts and ideas are presented, the authors succeed in clarifying in what respect they differ from previous work. A clearer separation of the description of previous work by other authors and the new contribution would be nice at some points, though.


Does it clearly indicate why this work is important?
----------------------------------------------------

NO. It does mention general game playing and real-time decision making in physical environments as possible application areas of the algorithms, and I am convinced that this is correct, but still this could be elaborated upon. E.g., why not present a real-world case study besides the benchmark domains?


Is the English satisfactory?
----------------------------

NO. At least one of the authors's English. It is possible to tell apart paragraphs by different authors by just observing whether the language is correct. The paragraphs not written by that particular author are fine. In the problematic paragraphs, the problem is mostly grammar (not vocabulary): definite and indefinite articles are incorrectly used and omitted, sometimes singular and plural are incorrectly used, plus some other minor mistakes. Fortunately, this only interferes with the reading flow but does not prevent understanding. Still, someone should re-read and correct the affected paragraphs.



Is the presentation otherwise satisfactory?
-------------------------------------------

YES.


Are there an appropriate number of figures?
-------------------------------------------

YES.


Do the figures help clarify the concepts in the paper?
------------------------------------------------------

YES. Absolutely.



*****************************
* Part B: DETAILED COMMENTS *
*****************************

The authors present several algorithms for the computation of strategies for two-player zero-sum simultaneous move extensive games, in particular backward induction (BI), BI\alpha\beta, Double Oracle (DO), DO\alpha\beta, and different variants of MCTS: decoupled UCT, Exp3, Regret Matching, CFR, and Online Outcome Sampling (OOS). After introduction, preliminaries, and discussion of related work, each of the algorithms is described in detail (including pseudocode).

After a brief discussion of offline vs. online search, the authors move to the experiments. They use six very different benchmark domains: Biased Rock-Paper-Scissors, Goofspiel, Oshi-Zumo, Pursuit-Evasion Games, Randomly generated games, and Tron. For each of the domains, the domain itself and the instance generation procedure are described, the importance of mixed (as opposed to pure) strategies in different stages of the game is explained, and a hand-tuned state evaluation function is provided as a heuristic function for some of the algorithms.

The first main part of the experiments is concerned with offline equilibrium computation. For each domain and each of the optimal algorithms (BI, BI\alpha\beta, DO, DO\alpha\beta), solving times for increasing-size instances are given, and for each domain and each of the sampling-based algorithms (MCTS-variants, OOS), figures showing convergence are provided. Typically, DO\alpha\beta and OOS perform best.

The second main part of the experiments is concerned with online search. Here, the authors provide head-to-head comparisons of their algorithms (plus UCT and a random player) on all domains for different time limits per move and with and without allowing the sampling-based algorithms to use the evaluation functions described before. Results are mixed, but among the optimal algorithms, typically DO\alpha\beta performs best, and with the evaluation function enabled, mostly  Regret Matching is the best-performing sampling-based approach. The mismatch of winners between the two main experiments (OOS vs. Random Matching) is briefly discussed. Finally, the authors give a short, moderately convincing account of their choice of parameter values of OOS, UCT, Exp3 and RM, then summarize and conclude.

Without going into details about how the presented algorithms work, it is clear that they make sense and behave correctly.

I really liked reading the paper.


Minor remarks:
--------------

* Description of Double-Oracle algorithm: I missed a remark about how much computational overhead is involved in incrementally updating LPs and solving them again. Also, I was wondering about how exactly you choose the initial restriction.

* Algorithms 3 and 4: BR(...) has different signatures in usage in Alg. 3 and definition in Alg. 4.

* Alg. 3: Before line 3, v_s^{i} and v_s^{-i} should somehow be initialized.

* Alg. 4: \lambda_{a_i} needs a clearer explanation.

* Alg. 4: Before line 10, v_{a_i, a_{-1}} should be initialized to 0 (I guess).

* Eq. 9: What does subscript "t" stand for?

* Eq. 9 ff: I am not sure of you use \sigma_i and \sigma_i^t consistently.

* Eq. 17: \pi_{-i}^{\sigma} and \pi^{\sigma} are undefined.

* Section 4.5.1 is a bit hard to understand in full detail. If there is anything in the paper I found really hard to grasp, then it was this section.

* Experiments: Usage of term "mixed strategy" confused me a bit, since each pure strategy is also a (trivial) mixed strategy.

* The evaluation function for Goofspiel looks wrong/weird. In the argument of tanh(), there are several divisions by zero that do actually occur at the beginning and the end of the game, respectively, and even if we interpret these situations in the limit, this would lead to eval-values of +/- 1 (after applying tanh), not 0, as claimed in the text for the beginning of the game. Uhm, okay, then the numerator is 0 as well. Anyway, this is somehow mathematically sloppy.



Reviewer #2: AIJ Review Form
===============

Part A
------

Please answer the following questions "yes/no" and provide appropriate justification as appropriate.

CONTENT

Is the paper technically sound?
Yes.

Is the work described original?
It is an overview of existing approaches.

Does it show sufficient applicability to AI?
Yes.

Does it place new results in appropriate context with earlier work?
Yes.

Is the paper sufficiently novel? (A paper is novel if the results it describes
were not previously published by other authors, and were not previously published
by the same authors in any archival journal.)
Yes (basically, it brings together the results of several conference
papers by the authors, but none of them were published in journals
before).

Is the paper complete? (Generally, a paper is complete if it includes all relevant proofs
and/or experimental data, a thorough  discussion of connections with the existing literature,
and a convincing discussion of the motivations and implications of the presented work.)
Yes.

Does anything need to be added or deleted?
Yes.

Is the result important enough to warrant publication?
Yes.

Accesibility
--
Is the paper accessible to the broad readers of AIJ?
Yes.

Specifcally:

Is the problem addressed in the paper, described in an accessible manner to readers of the journal?
Yes.

Is it possible for an AI researcher, outside the specific area,  to assess its relevance to  AI in general and  to their own research area, in particular?
Yes.

Is enough background given (e.g., prior work, preliminaries, methodology) so an AI researcher outside the specific area can  appreciate the paper's contribution?
Largely yes, but some basics are missing, e.g., the notion of Nash
equilibria and the concept of dominance have not been introduced when
they are used.
----

FORM

Does the abstract adequately reflect the contents?
Yes.
Does it contain adequate references to previous work?
Yes.
Does it explain clearly what was done that is new?
Yes.
Does it clearly indicate why this work is important?
Yes.
Is the English satisfactory?
No.
Is the presentation otherwise satisfactory?
No. There are many small inconsistencies in the formalizations.
Are there an appropriate number of figures?
Yes.
Do the figures help clarify the concepts in the paper?
Yes.



Part B: DETAILED COMMENTS
-------------------------

The basic outset of the paper is to give an overview of recent
approaches for (a) solving (i.e., calculating Nash equilibria) and (b)
playing two-player simultaneous-move games (possibly with a third
player, i.e., chance). The presented algorithms have been proposed by
the authors (some of the algorithms also by other researchers) before
in a number of conference publications. They contain algorithms based
on backward induction and algorithms based on Monte-Carlo tree search
(MCTS). The former can solve games much faster than the latter can
converge, while the latter typically result in better performance in
the online setting of actually playing the game.

For the exact methods, a simple backward induction algorithm is
extended by (a) serializing the game to get upper and lower bounds,
determined by running Alpha-Beta on the serialized games; if the
bounds equal, the simultaneous-move backward induction does not have
to be executed. (b) performing a so-called double-oracle algorithm
(possibly also together with the serialization approach).

For the MCTS based approaches, the older approaches UCT and Exp3 as
well as the authors' Regret Matching and Online Outcome Sampling are
introduced and evaluated. One of the problems of basic UCT is that it
will not converge to a mixed Nash equilibrium, so that some extensions
(like randomizing the selection step and choosing the actual move to
play by a kind of mixed strategy) are explained.

In the experiments, the algorithms are compared in terms of offline
and online behavior for six different games. It turns out that
overall, the Alpha-Beta based exact solvers find the solutions much
faster than the simulation based approaches converge in the online
setting. In the offline setting, however, in most cases the simulation
based approaches result in much better performance.



Unfortunately, the article suffers severely from the language,
especially considering it is a submission to AIJ. The entire article
should be carefully proofread -- by a native speaker if at all
possible. Among others, often there are missing articles, confusion of
singular and plural, and many other small things. None of these are so
bad as to prevent understanding of the overall text, but still this is
far from the standards I expect from an AIJ article.

Some of the core concepts are not properly introduced. True, they
should be fairly well known by anyone working in this area. However,
in my opinion a journal article should be as self-contained as
possible. Especially introductions into the concept of Nash equilibria
and dominance should be given.
Many concepts in Section 3 (Related Work) are also only barely
scratched, but here it is OK, as they are not part of the actual
article.

Concerning the experiments, I am surprised to see that in nearly all
the games the rewards are set to {-1,1} or {-1,0,1}, although more
general rewards would clearly have been possible (e.g., in Goofspiel
or Oshi-Zumo). Is there an explanation for this? Does this kind of
rewards make life easier for the exact or the approximative
approaches? What would happen if the rewards were taken from much
larger sets?

Finally, there are many small mistakes and inconsistencies in the
formalizations and the algorithms. I will point out many of those in
what follows.


Section 2:
I get it why for A_{rc} r (row) and c (column) are chosen; however,
before c was defined as the chance player, so this might cause some
confusion.

Notion of dominance and Nash equilibria used (on p. 4) but not
properly introduced.

In Eq. (1): What is Z? Is it the set of terminal states? Those were
defined with a different font style.

"It guarantees the payoff of at least V*" -> Is that so? I thought
that's only the expected outcome. So, in a single game the actual
payoff may be much lower.

Fig. 2 referenced much later (after Fig. 3). Also, the entire
paragraph (i.e., 3 lines) referencing this are not much help. Either
properly introduce the concept of two-player finite-horizon Markov
Games, or maybe think about dropping this paragraph.



Section 4:
Overall, this is a nice introduction into all the used algorithms,
providing most of the necessary background.


Section 4.1:
In the text I actually stumbled over the meaning of T(s,r,c). Again,
using c as the name for the chance player and the name for player 2
(column) is extremely confusing.

In the definition on page 4, \Delta_c is defined over \cal S \mapsto
\Delta(\cal S). However, in Section 4.1, \cal S \times \cal S \mapsto
\Delta(\cal S) is used (i.e., current and successor state are taken as
input). This seems to make more sense, but it has to be made
consistent throughout the article, especially so that it matches the
original definition (which seems to be what is wrong here).

In Alg. 1, line 5: What is s' \in S; especially: what is S? If it is
the set of all possible states, that was defined as \cal S on page 3
-- note the different font! (same in Alg. 2+3+4)

In equation (3): What is A_{-i}? Is it supposed to be A_{3-i}, i.e.,
the actions of the opponent of player i? (same throughout the
remainder of the article)

In equations (3), (4), and (5): What is \delta^s_i?


Section 4.2:
How is the actual serialization performed for a multi-step game (as
opposed to a simple matrix game)? That is, is it enough to consider
the two cases where player 1 is always first or always second, or is
it necessary to do this serialization in every state, i.e., in each
state consider the two possibilities? The latter sounds way too
expensive, but I would have expected to see some argumentation why the
former is enough to get proper bounds.


Section 4.3:
In Alg. 3, "solve matrix game" returns a pair (the value and \delta,
which seems to be the mixed equilibrium strategy); before "solve
matrix game" returned only the value -- this should be made consistent
(e.g., call it somewhat differently in Alg. 3 to make it clear that
this is based on more than just the LP in equations (2) to (5).

In Alg. 4, BS takes the current state as first argument; this is not
passed in Alg. 3. Also the order of the strategy and the bound is
different in the two algorithms. Overall, BS in Alg. 4 takes 4
arguments, while its call in Alg. 3 has only 3.

Alg. 4, l. 12: What is v_{a_i,a_{-i}}? This is used here for the first
time, but its new value is dependent on its old value, so this must be
initialized somewhere.

In the description of Alg. 4: "until the game for state s is not
solved" -> should be until it is solved


Section 4.4:
Fig. 6 is supposed to be a possible result after running MCTS for four
iterations on the game from Fig. 2. However, the subgames after
playing (t,l) and (b,l) are simple matrix games. For the latter, the
top-left cell has a fixed value of 0 in Fig. 2, but in Fig. 6 we
somehow could get a value of 1 -- this should be made consistent.

In equation (9): What does the t in \sigma^t_i stand for?

In equation (9): The summand \gamma / |\cal A_i(s)| is missing; this
way, equation (9) is not equivalent to equation (8), as is stated in
the text.

In equation (10): What are v_{max} and v_{min}?

In equation (11): Should it not be \sigma^t_i(a_i)?

In equation (12): Should it not be \bar\sigma_i(a_i) (i.e., without
^t)?

At the bottom of page 24: with probability \gamma / |\cal A(s)|...
Should it not be \cal A_i(s)?

In equation (17): What is \pi^\sigma_{-i} of a history? What is
\pi^\sigma of two histories, where the first is a prefix of the
second?

Right after equation (17): z \in Z -> should it be z \in \cal Z? If
so, how can a history be in the set of terminal states?

Next line: strategy \sigma_i^t is defined; the next sentence uses
\sigma^t (i.e., without the _i). Is this correct?

a \in A(I) -> what is A(I)?

In equation (18), denominator: should be \sum_{t=1}^T (^T is missing)

Bottom of page 26: R^T(I,a) is defined "for information set I of
player i in state s" -> I guess, i should be mentioned somewhere in
R^T.


Section 6.2:
In Goofspiel, with the assumption that the players know the sequence
of nature's cards, the chance player is basically removed, right? In
the actual experiments, a decreasing order of nature's cards is
mentioned (e.g., in Tab. 1). This sounds like a very simplifying
restriction. Why was this made?

What is the branching factor of a simultaneous move game? I had
assumed it to be the branching factor of the joint actions, but from
the descriptions (especially in Goofspiel) it seems that rather it is
the number of actions available to one of the players in a given
state.

In Oshi-Zumo, it is said that the number of coins N and the size of
the playing field 2K+1 are varied. What about the minimum bid M?

In the evaluation function for Oshi-Zumo: There are two cases for
which the value of b is specified; what about the other cases?

In Tron, what are the rewards?


Section 6.4.5:
Why is there no graph for the comparison of the exact algorithms in
Tron?


Section 6.5.2:
The authors claim that RM clearly dominates all other approaches.
However, according to the table this is not always the case. In fact,
I am surprised to see that in all columns RM is marked as being the
best, though that clearly is not the case in some of the cases (E.g.,
against DO\alpha\beta EXP3 shows better performance, and against
random DO\alpha\beta is best in the decreasing cards settings).

I am surprised by the results of the random player overall. Basically,
it seems that DO\alpha\beta is the worst approach, OOS slightly better
and RM the best (in most cases). However, against random this trend is
inverted. Is there an explanation why the worse players fare best
against a random player?


Section 6.5:
Is there an explanation why the good convergence of OOS does not
translate to good performance in online play? E.g., in Section 6.5.5
it is said that the good performance of RM is consistent with the
offline convergence results. However, in every single domain it is
said that OOS, which converged fastest in most cases, is the worst
approach in the online setting.



The tables in Section 6.5 should be explained in more detail.
Especially, it should be noted much earlier that the rows stand for
player 1 and the columns for player 2. While this is not relevant in
most games, in the pursuit-evasion games there is a huge bias towards
the pursuer. These things are only discussed in the context of Table
5. Apart from the pursuit-evasion games, is there any such bias in the
other games as well? Clearly not in Goofspiel and Oshi-Zumo, but in
Tron it might depend on the actual starting positions, and in the
random games I have no intuitions on this at all.


In the parameter-tuning experiments, according to Tab. 6, the
performance of Exp3 and RM was still improving with increasing
parameter value. Thus, it might make sense to test even higher values
for these two approaches.

Also, I find it surprising to see that RM performs better against
DO\alpha\beta when a higher exploration factor is used, but in
self-play such a high factor is much worse than a lower one; also, the
higher value results in worse performance against the other
simulation-based approaches. Is there any explanation for this
behavior?



In all the pseudocodes:
Sometimes a line ends with a semicolon; sometimes it does not. This
should be made consistent.



The references should be made consistent. One thing I noticed is that,
e.g., Czech, accents are not always there. This holds among others for
one of the authors of this article: In papers 6 and 8 Viliam Lisy
comes without the accent over the y, while in paper 7 it is there.
Furthermore, sometimes editors are mentioned, sometimes not; sometimes
publishers are mentioned, sometimes not; etc.




Overall, in my opinion this is a nice overview paper over current
approaches for solving and playing two-player simultaneous move games,
possibly including chance events. Unfortunately, it suffers a lot from
the language and also from a huge number of rather smallish mistakes
and inconsistencies. The overview of the algorithms is fine (apart
from said mistakes), and the experiments give rather clear results.
However, for the experiments I would have liked to see different
reward functions, not just won, lost, and maybe draw. Also, I am not
sure if six domains is a big enough set, but given that the games have
quite different characteristics, I do not see this as a severe
shortcoming. The paper clearly needs a serious rewrite, but in my
opinion it will be possible to end up with a nice article after all.

